# Query Agent Architecture: RAG-Based Query Reuse

## Core Insight

**Reusing validated queries stabilizes entropy faster than generating fresh SQL.**

Traditional approaches have the LLM generate SQL from scratch for each question. This means:
- Every query is "novel" from an entropy perspective
- No learning accumulates across sessions
- Same mistakes get repeated
- Users don't trust results

The DataRaum approach inverts this:
- Build a library of **validated query patterns**
- Search by semantic similarity when users ask questions
- Adapt existing patterns rather than generating from scratch
- Only generate new SQL for genuinely novel patterns
- Validated queries have **known entropy profiles**

---

## The Query RAG Flywheel

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  User asks question â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Search query libraryâ”‚
                    â”‚  (semantic similarity)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Match found:     â”‚            â”‚  No match:          â”‚
    â”‚  Adapt existing   â”‚            â”‚  Generate new SQL   â”‚
    â”‚  query            â”‚            â”‚  (with entropy      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  awareness)         â”‚
              â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                                 â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Execute & validate â”‚
                    â”‚  (user feedback,    â”‚
                    â”‚  result inspection) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Save/update libraryâ”‚
                    â”‚  (entropy stabilizesâ”‚
                    â”‚  with reuse)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Works

| Approach | Entropy Behavior |
|----------|------------------|
| **Generate CTEs each time** | Every query is novel â†’ entropy stays high, no learning |
| **RAG of tested queries** | Reuse validated patterns â†’ entropy drops with each use |

The flywheel effect:
1. More queries in library â†’ better retrieval
2. Better retrieval â†’ more reuse
3. More reuse â†’ queries get validated/refined
4. Refined queries â†’ lower entropy profiles
5. Lower entropy â†’ higher user trust â†’ more usage

---

## Query Library Schema

Each query entry contains:

```yaml
query_id: "revenue_by_region_monthly"
version: 3

# Semantic metadata for RAG retrieval
description: "Monthly revenue breakdown by sales region"
business_context:
  metric: "revenue"
  dimensions: ["region", "month"]
  filters: ["active_customers_only"]
  industry: "finance"

# The actual SQL
sql: |
  SELECT
    region,
    DATE_TRUNC('month', order_date) as month,
    SUM(amount) as revenue
  FROM typed_orders
  WHERE customer_status = 'active'
  GROUP BY 1, 2

# Lineage (which tables/columns touched)
dependencies:
  tables: ["typed_orders"]
  columns: ["region", "order_date", "amount", "customer_status"]

# Validated entropy profile
entropy_profile:
  currency: 0.2      # validated: all EUR
  temporal: 0.1      # validated: calendar months
  null_handling: 0.3 # some nulls in region, documented
  overall: 0.2

# Assumptions baked into this query
assumptions:
  - basis: "system_default"
    description: "Revenue in EUR"
  - basis: "inferred"
    description: "Excluding cancelled orders"

# Usage tracking
validated_at: "2026-01-15"
validated_by: "analyst@company.com"
usage_count: 47
last_used: "2026-01-22"

# For graph visualization
calculation_graph:
  - step: "filter"
    description: "Active customers only"
  - step: "aggregate"
    description: "Sum amount by region and month"
```

---

## The Graph View's Dual Purpose

The web_visualizer prototype shows calculation graphs: how metrics are built from the typed data layer through aggregations to final metrics.

**Purpose 1: Human Understanding**
- Visualize data lineage
- Understand how metrics are calculated
- See which columns contribute to which outputs
- Drill down into calculation steps

**Purpose 2: Query Library Seeding**
- Each graph IS a validated SQL pattern
- Graphs come with semantic tags (metric name, dimensions, business context)
- The visualization provides the "documentation" users need to trust the query
- Expand/collapse reveals calculation breakdown

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MetricNode                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Monthly Revenue by Region                       â”‚   â”‚
â”‚  â”‚  â‚¬12.1M                                         â”‚   â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚   â”‚
â”‚  â”‚  â–¼ Expand calculation                           â”‚   â”‚
â”‚  â”‚    â”œâ”€ SUM(amount) from typed_orders            â”‚   â”‚
â”‚  â”‚    â”œâ”€ Filtered: customer_status = 'active'     â”‚   â”‚
â”‚  â”‚    â”œâ”€ Grouped: region, month                   â”‚   â”‚
â”‚  â”‚    â””â”€ Entropy: 0.2 (ready)                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Existing Modules and Adaptation Needed

### `graphs/` Module (Current State)

The existing graphs module was designed for:
- Pre-defined calculation graphs based on industry-standard metrics
- Financial metrics (revenue, costs, margins, etc.)
- Graph execution with filter application
- Entropy-aware query behavior

**Adaptation needed:**
- Add semantic embeddings for RAG retrieval
- Store validated entropy profiles per graph
- Track usage and validation history
- Support graph composition (combining building blocks)

### `web_visualizer` Prototype (Current State)

Shows:
- MetricNode, FieldNode, AccountNode, TransactionNode
- Expand/collapse for calculation breakdown
- Drill-down modals with sortable tables
- Sidebar navigation between views

**Adaptation needed:**
- Connect to live API (currently uses mock data)
- Add entropy indicators to nodes
- Enable "save as query" workflow
- Support query refinement UI

### `analytics-agents-ts` Prompts (Current State)

Contains sophisticated prompt patterns for:
- SQL generation with DuckDB standards
- Multiple table outputs (executive, analyst, operational)
- Chart type recommendations
- Data quality consideration

**Adaptation needed:**
- Integrate with query library search
- Add "adapt existing query" mode
- Include entropy context in prompts
- Support iterative refinement workflow

---

## Query Agent Workflow

### Step 1: Question Understanding

User asks: "What was our revenue by region last quarter?"

Agent extracts:
- **Metric**: revenue
- **Dimensions**: region
- **Time filter**: last quarter
- **Implicit**: probably wants comparison, totals

### Step 2: Library Search

Search query library with:
```
metric:revenue dimensions:region temporal:quarterly
```

Results:
1. `revenue_by_region_monthly` (0.92 similarity) - adapt time grouping
2. `revenue_by_region_ytd` (0.85 similarity) - different time scope
3. `revenue_total_quarterly` (0.78 similarity) - missing region dimension

### Step 3: Adaptation or Generation

**If good match found:**
```sql
-- Adapted from: revenue_by_region_monthly (v3)
-- Change: DATE_TRUNC('quarter', ...) instead of 'month'
-- Entropy profile: inherited (0.2)
SELECT
  region,
  DATE_TRUNC('quarter', order_date) as quarter,
  SUM(amount) as revenue
FROM typed_orders
WHERE customer_status = 'active'
  AND order_date >= DATE_TRUNC('quarter', CURRENT_DATE - INTERVAL '3 months')
GROUP BY 1, 2
```

**If no match (novel pattern):**
Generate with full entropy awareness, flag for validation.

### Step 4: Execution with Entropy Context

Execute query and include:
- Assumptions made (inherited from base query + new ones)
- Entropy warnings if any dimensions are uncertain
- Confidence level based on entropy profile

### Step 5: Result Presentation

Following analytics-agents-ts patterns:
- Multiple table views (executive summary, detailed breakdown)
- Chart recommendations based on data shape
- Assumptions clearly stated
- Option to save as new query or update existing

### Step 6: Feedback Loop

User can:
- **Validate**: "This is correct" â†’ increases confidence
- **Refine**: "Actually, exclude APAC" â†’ creates variant
- **Report issue**: "Numbers don't match" â†’ flags for review
- **Save**: Add to library with description

---

## End-to-End Validation Questions

Before building elaborate UI, validate these assumptions:

1. **Can semantic search find relevant queries?**
   - Test: Embed 50 sample queries, search with natural language
   - Success: >80% of questions find relevant match in top 3

2. **Does query reuse actually stabilize entropy?**
   - Test: Track entropy scores over time for reused vs. novel queries
   - Success: Reused queries show lower variance in results

3. **Can users understand the graph view?**
   - Test: Show calculation graph, ask users to explain what it does
   - Success: Users can correctly describe the calculation

4. **Does the RAG approach feel faster/better to users?**
   - Test: A/B test RAG vs. pure generation
   - Success: RAG queries have higher user satisfaction

---

## Implementation Phases

### Phase 1: API Integration (Current)
- Complete context/entropy/graphs endpoint integration
- Validate existing data structures work end-to-end
- Ensure `GraphExecutionContext` and `EntropyContext` are correct

### Phase 2: Query Library Foundation
- Design query library schema (extend existing graphs/)
- Add semantic embedding storage
- Implement basic similarity search
- Seed with existing graph definitions

### Phase 3: Query Agent MVP
- Implement search â†’ adapt â†’ execute flow
- Basic entropy-aware query generation
- Simple result presentation (tables only)
- Assumption tracking

### Phase 4: Graph Visualization
- Adapt web_visualizer to live API
- Add entropy indicators
- Enable drill-down into calculations
- Connect graph view to query library

### Phase 5: Interactive Refinement
- SQL refinement workflow
- Chart recommendations
- Save/update query workflow
- User feedback collection

### Phase 6: Configuration Hub
- Only after validating core assumptions
- Threshold tuning based on real usage patterns
- Semantic override UI
- Contract configuration

---

## Success Metrics

| Metric | Target | Why It Matters |
|--------|--------|----------------|
| Query reuse rate | >60% | Validates RAG approach |
| Avg entropy of executed queries | <0.4 | Shows stabilization |
| User trust (survey) | >4/5 | End goal |
| Time to answer | <10s for reused | UX target |
| Query library growth | +10/week | Flywheel working |

---

## Related Documents

- [ui-api-consolidation.md](./ui-api-consolidation.md) - Implementation status and phases
- [ENTROPY_QUERY_BEHAVIOR.md](../ENTROPY_QUERY_BEHAVIOR.md) - Agent response policies
- [ENTROPY_CONTRACTS.md](../ENTROPY_CONTRACTS.md) - Data readiness thresholds
- [ENTROPY_IMPLEMENTATION_PLAN.md](../ENTROPY_IMPLEMENTATION_PLAN.md) - Entropy system design
- [BACKLOG.md](../BACKLOG.md) - Task tracking

---

## Contract-Based Confidence Levels

### The Traffic Light Model

When a user queries with a contract, the response includes a **confidence level** based on how well the data meets the contract's entropy thresholds:

| Level | Color | Meaning | Agent Behavior |
|-------|-------|---------|----------------|
| ðŸŸ¢ **GREEN** | Green | All thresholds pass | Answer confidently |
| ðŸŸ¡ **YELLOW** | Yellow | Minor issues (within 20% of threshold) | Answer with minor caveats |
| ðŸŸ  **ORANGE** | Orange | Significant issues (some dimensions exceed) | Answer with strong caveats, list issues |
| ðŸ”´ **RED** | Red | Critical issues (blocking violations) | Refuse or require confirmation |

### Confidence Calculation

```python
def calculate_confidence_level(
    evaluation: ContractEvaluation,
) -> ConfidenceLevel:
    """Calculate traffic light confidence from contract evaluation."""

    if evaluation.is_compliant:
        # Check if any dimensions are close to threshold (within 20%)
        near_threshold = [
            v for v in evaluation.warnings
            if v.actual > v.max_allowed * 0.8
        ]
        if near_threshold:
            return ConfidenceLevel.YELLOW
        return ConfidenceLevel.GREEN

    # Non-compliant: check severity
    blocking = evaluation.get_blocking_violations()

    if blocking:
        # Has blocking violations
        if any(v.severity == "critical" for v in blocking):
            return ConfidenceLevel.RED
        return ConfidenceLevel.ORANGE

    # Non-blocking violations only
    return ConfidenceLevel.YELLOW
```

### Response Format with Confidence

```
ðŸŸ¢ Data Quality: GOOD for executive_dashboard

Revenue by region for Q3 2024:
| Region   | Revenue |
|----------|---------|
| EMEA     | â‚¬4.2M   |
| APAC     | â‚¬2.8M   |
| Americas | â‚¬5.1M   |

Total: â‚¬12.1M
```

```
ðŸŸ  Data Quality: ISSUES for executive_dashboard

Revenue by region for Q3 2024:
| Region   | Revenue |
|----------|---------|
| EMEA     | â‚¬4.2M   |
| APAC     | â‚¬2.8M   |
| Americas | â‚¬5.1M   |

Total: â‚¬12.1M

âš ï¸ Quality Issues:
- Currency: 45% uncertain (threshold: 20%) â€” assuming EUR
- Temporal: 35% uncertain (threshold: 30%) â€” assuming calendar quarter

To improve: Add currency field to orders table, or configure default currency.
```

```
ðŸ”´ Data Quality: BLOCKED for regulatory_reporting

I cannot provide a reliable answer for regulatory_reporting because:

âŒ Currency entropy: 0.72 (max allowed: 0.10)
   - 40% of revenue records have no currency indicator

âŒ Temporal alignment: 0.55 (max allowed: 0.10)
   - Mixed calendar/fiscal periods detected

To proceed:
1. Choose a different contract (e.g., exploratory_analysis)
2. Or resolve these issues first

Would you like me to answer with exploratory_analysis contract instead?
```

### Contract Selection in Queries

Users can specify a contract explicitly or let the system choose a default:

```bash
# Explicit contract
dataraum query "What was revenue?" --contract regulatory_reporting

# Default contract (exploratory_analysis)
dataraum query "What was revenue?"

# Ask system to recommend appropriate contract
dataraum query "What was revenue?" --auto-contract
```

With `--auto-contract`, the system evaluates all contracts and recommends the strictest one that passes:

```
ðŸ“Š Contract Recommendation:

Your data quality supports:
  ðŸŸ¢ exploratory_analysis â€” PASS
  ðŸŸ¢ operational_analytics â€” PASS
  ðŸŸ¡ executive_dashboard â€” MARGINAL (1 warning)
  ðŸ”´ regulatory_reporting â€” FAIL (3 blocking issues)

Using: executive_dashboard (strictest passing contract)

[Answer follows...]
```

---

## Architecture: Library-First Design

### Core Principle

The query agent is implemented as a **library function** that both CLI and API call directly. This ensures:
- Single code path for all interfaces
- Faster iteration (no server restart)
- Easier testing and debugging

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼             â–¼             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   CLI   â”‚  â”‚   API    â”‚  â”‚   MCP   â”‚
   â”‚ dataraumâ”‚  â”‚ FastAPI  â”‚  â”‚ Server  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚            â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     Query Agent         â”‚
        â”‚   (Library Function)    â”‚
        â”‚                         â”‚
        â”‚  answer_question()      â”‚
        â”‚  evaluate_contract()    â”‚
        â”‚  search_query_library() â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼            â–¼            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Entropy â”‚ â”‚  Query   â”‚ â”‚ Context  â”‚
   â”‚ Context â”‚ â”‚ Library  â”‚ â”‚ Builder  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  ConnectionManager      â”‚
        â”‚  (metadata.db +         â”‚
        â”‚   data.duckdb)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Library Function

```python
# src/dataraum/query/agent.py

@dataclass
class QueryResult:
    """Result from query agent."""

    # The answer
    answer: str                      # Natural language response
    sql: str | None                  # Generated/adapted SQL
    data: list[dict] | None          # Query results as records

    # Confidence and quality
    confidence_level: ConfidenceLevel  # GREEN/YELLOW/ORANGE/RED
    entropy_score: float               # Overall entropy for this query
    assumptions: list[QueryAssumption] # Assumptions made

    # Contract evaluation (if contract specified)
    contract: str | None               # Contract name used
    contract_evaluation: ContractEvaluation | None

    # Query library tracking
    source_query_id: str | None        # If adapted from library
    is_novel: bool                     # True if newly generated

    # Metadata
    execution_id: str
    executed_at: datetime


def answer_question(
    question: str,
    manager: ConnectionManager,
    source_id: str,
    *,
    contract: str | None = None,       # Explicit contract
    auto_contract: bool = False,       # Find best contract
    behavior_mode: str = "balanced",   # strict/balanced/lenient
    save_to_library: bool = True,      # Save successful queries
) -> QueryResult:
    """Answer a question using the query agent.

    This is the core library function called by CLI, API, and MCP.

    Flow:
    1. Build execution context with entropy
    2. Search query library for similar questions
    3. If match found: adapt existing query
    4. If no match: generate new SQL with entropy awareness
    5. Execute query
    6. Evaluate against contract (if specified)
    7. Format response with confidence level
    8. Optionally save to query library
    """
```

### File Structure

```
src/dataraum/
â”œâ”€â”€ cli.py                    # Extend with query commands
â”œâ”€â”€ query/                    # NEW: Query agent module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent.py              # answer_question() - core function
â”‚   â”œâ”€â”€ library.py            # Query library (store, search, adapt)
â”‚   â”œâ”€â”€ confidence.py         # Confidence level calculation
â”‚   â”œâ”€â”€ models.py             # QueryResult, QueryAssumption, etc.
â”‚   â””â”€â”€ db_models.py          # QueryLibraryEntry, QueryExecution
â”œâ”€â”€ api/routers/
â”‚   â””â”€â”€ query.py              # Extend with /query/agent endpoint
â”œâ”€â”€ entropy/
â”‚   â””â”€â”€ contracts.py          # Contract evaluation (implement from spec)
â””â”€â”€ mcp/
    â””â”€â”€ tools/
        â””â”€â”€ query.py          # MCP tool wrapping answer_question()
```

---

## Test Flow

### Complete End-to-End Test Scenario

```bash
# ============================================
# PHASE 1: Data Import
# ============================================

# Import test dataset
dataraum run ./examples/data/financial --output ./test_output

# Verify import succeeded
dataraum status ./test_output
# Output:
#   Source: financial (abc123...)
#   Tables: 5 (raw: 5, typed: 5, quarantine: 0)
#   Rows: 10,234
#   Phases: 18 completed

# Inspect context and entropy
dataraum inspect ./test_output
# Output:
#   Entropy Summary:
#     Overall: 0.42 (INVESTIGATE)
#     Highest: semantic.units (0.65)
#     Compound Risks: 1 (currency + aggregation)

# ============================================
# PHASE 2: Contract Evaluation
# ============================================

# Check which contracts this data supports
dataraum contracts ./test_output
# Output:
#   Contract Compliance:
#   ðŸŸ¢ exploratory_analysis    â€” PASS (all thresholds met)
#   ðŸŸ¢ data_science            â€” PASS (all thresholds met)
#   ðŸŸ¡ operational_analytics   â€” MARGINAL (1 warning)
#   ðŸŸ  executive_dashboard     â€” ISSUES (2 violations)
#   ðŸ”´ regulatory_reporting    â€” BLOCKED (5 violations)

# Get details for a specific contract
dataraum contracts ./test_output --contract executive_dashboard
# Output:
#   Contract: executive_dashboard
#   Status: ðŸŸ  NON-COMPLIANT
#
#   Violations:
#     âŒ semantic.units: 0.65 (max: 0.20)
#        Affected: revenue, costs, margin
#     âŒ semantic.temporal: 0.35 (max: 0.30)
#        Affected: order_date
#
#   Path to Compliance:
#     1. [LOW effort] Add currency to orders table
#        Expected reduction: -0.40 on semantic.units
#     2. [LOW effort] Configure fiscal calendar
#        Expected reduction: -0.10 on semantic.temporal

# ============================================
# PHASE 3: Query Agent Testing
# ============================================

# Simple query with default contract (exploratory)
dataraum query "How many orders are there?" -o ./test_output
# Output:
#   ðŸŸ¢ Data Quality: GOOD for exploratory_analysis
#
#   There are 5,234 orders in the dataset.

# Query with medium entropy
dataraum query "What was total revenue last month?" -o ./test_output
# Output:
#   ðŸŸ¢ Data Quality: GOOD for exploratory_analysis
#
#   Total revenue last month: â‚¬1,247,832
#
#   Assumptions:
#   - Currency: EUR (inferred from majority of records)
#   - Period: Calendar month (December 2025)

# Query with stricter contract
dataraum query "What was revenue by region?" -o ./test_output \
    --contract executive_dashboard
# Output:
#   ðŸŸ  Data Quality: ISSUES for executive_dashboard
#
#   Revenue by region:
#   | Region   | Revenue   |
#   |----------|-----------|
#   | EMEA     | â‚¬4,234,123|
#   | APAC     | â‚¬2,891,456|
#   | Americas | â‚¬5,102,789|
#
#   âš ï¸ Quality Issues:
#   - Currency: 65% uncertain (threshold: 20%)
#     Assuming EUR based on system default
#
#   To improve confidence:
#   - Add currency field to orders table

# Query that would be blocked
dataraum query "What was revenue?" -o ./test_output \
    --contract regulatory_reporting
# Output:
#   ðŸ”´ Data Quality: BLOCKED for regulatory_reporting
#
#   Cannot provide answer for regulatory reporting because:
#
#   âŒ semantic.units: 0.65 (max: 0.10)
#   âŒ semantic.temporal: 0.35 (max: 0.10)
#   âŒ computational.aggregations: 0.25 (max: 0.10)
#
#   Options:
#   1. Use --contract exploratory_analysis (will show answer with caveats)
#   2. Resolve data quality issues first
#
#   Run `dataraum contracts ./test_output --contract regulatory_reporting`
#   to see path to compliance.

# Auto-contract selection
dataraum query "What was profit margin?" -o ./test_output --auto-contract
# Output:
#   ðŸ“Š Contract: operational_analytics (auto-selected)
#
#   Your data supports up to operational_analytics
#   (executive_dashboard has 2 violations)
#
#   ðŸŸ¢ Data Quality: GOOD for operational_analytics
#
#   Profit margin: 23.4%
#
#   Calculation: (Revenue - Costs) / Revenue
#   Based on: 5,234 orders

# ============================================
# PHASE 4: Interactive Mode
# ============================================

dataraum query --interactive -o ./test_output
# Output:
#   DataRaum Query Agent
#   Type 'help' for commands, 'exit' to quit
#   Contract: exploratory_analysis (default)
#
#   > What tables are available?
#   ðŸŸ¢ Tables: orders, customers, products, regions, costs
#
#   > /contract executive_dashboard
#   Contract set to: executive_dashboard
#   âš ï¸ Note: Current data has 2 violations for this contract
#
#   > What was revenue by product category?
#   ðŸŸ  Data Quality: ISSUES for executive_dashboard
#   [... response ...]
#
#   > /save "revenue_by_category"
#   Query saved to library as: revenue_by_category
#
#   > exit
```

### CLI Commands Summary

| Command | Purpose |
|---------|---------|
| `dataraum run SOURCE` | Import data, run pipeline |
| `dataraum status DIR` | Show pipeline status |
| `dataraum inspect DIR` | Show context and entropy |
| `dataraum contracts DIR` | Evaluate all contracts |
| `dataraum contracts DIR --contract NAME` | Evaluate specific contract |
| `dataraum query "..." -o DIR` | Ask a question |
| `dataraum query "..." --contract NAME` | Ask with specific contract |
| `dataraum query "..." --auto-contract` | Auto-select best contract |
| `dataraum query --interactive` | Interactive REPL mode |

### API Endpoints Summary

| Endpoint | Purpose |
|----------|---------|
| `POST /api/v1/query/agent` | Answer a question |
| `GET /api/v1/contracts` | List all contracts |
| `GET /api/v1/contracts/{name}` | Get contract definition |
| `GET /api/v1/contracts/{name}/evaluate` | Evaluate contract |
| `GET /api/v1/query/library` | List saved queries |
| `POST /api/v1/query/library` | Save query to library |

"""Type resolution engine - DuckDB SQL generation.

Generates SQL to create typed tables from raw VARCHAR tables
using TypeCandidates computed during type inference.

The quarantine pattern:
- Rows where ANY column fails TRY_CAST go to quarantine table
- This allows downstream processing on clean typed data
- Quarantined rows can be reviewed and fixed
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import UTC, datetime
from uuid import uuid4

import duckdb
from sqlalchemy import select
from sqlalchemy.orm import Session, selectinload

from dataraum.analysis.typing.db_models import TypeCandidate, TypeDecision
from dataraum.analysis.typing.models import ColumnCastResult, TypeResolutionResult
from dataraum.analysis.typing.patterns import Pattern, load_pattern_config
from dataraum.core.models.base import ColumnRef, DataType, Result
from dataraum.storage import Column, Table


@dataclass
class ColumnTypeSpec:
    """Type specification for a column during resolution."""

    column_id: str
    column_name: str
    data_type: DataType
    pattern: Pattern | None = None
    decision_source: str = "automatic"  # 'automatic', 'manual', 'override', 'fallback'
    decision_reason: str | None = None
    candidate_confidence: float | None = None  # Confidence from best TypeCandidate


def _select_best_candidates(
    columns: list[Column],
    min_confidence: float,
) -> list[ColumnTypeSpec]:
    """Select best type candidate per column.

    Priority:
    1. TypeDecision (human override) if exists
    2. Highest confidence TypeCandidate >= threshold
    3. Fallback to VARCHAR

    Returns ColumnTypeSpec with decision metadata for persisting TypeDecision records.
    """
    pattern_config = load_pattern_config()
    patterns_by_name = {p.name: p for p in pattern_config.get_patterns()}
    specs = []

    for col in sorted(columns, key=lambda c: c.column_position):
        # Check for human override (pre-existing TypeDecision)
        if col.type_decision:
            specs.append(
                ColumnTypeSpec(
                    column_id=col.column_id,
                    column_name=col.column_name,
                    data_type=DataType[col.type_decision.decided_type],
                    decision_source="manual",  # Already decided by human
                    decision_reason=col.type_decision.decision_reason,
                )
            )
            continue

        # Find best candidate
        candidates = sorted(col.type_candidates, key=lambda c: c.confidence, reverse=True)
        if candidates and candidates[0].confidence >= min_confidence:
            best = candidates[0]
            pattern = patterns_by_name.get(best.detected_pattern) if best.detected_pattern else None
            specs.append(
                ColumnTypeSpec(
                    column_id=col.column_id,
                    column_name=col.column_name,
                    data_type=DataType[best.data_type],
                    pattern=pattern,
                    decision_source="automatic",
                    decision_reason=f"Best candidate with confidence {best.confidence:.2f} (pattern: {best.detected_pattern or 'none'})",
                    candidate_confidence=best.confidence,
                )
            )
        else:
            # Fallback to VARCHAR
            best_conf = candidates[0].confidence if candidates else 0.0
            specs.append(
                ColumnTypeSpec(
                    column_id=col.column_id,
                    column_name=col.column_name,
                    data_type=DataType.VARCHAR,
                    decision_source="fallback",
                    decision_reason=f"No candidate met confidence threshold {min_confidence} (best: {best_conf:.2f})",
                    candidate_confidence=best_conf if candidates else None,
                )
            )

    return specs


def _generate_typed_table_sql(
    raw_table: str,
    typed_table: str,
    specs: list[ColumnTypeSpec],
) -> str:
    """Generate CREATE TABLE with TRY_CAST per column."""
    selects = []
    for spec in specs:
        col = f'"{spec.column_name}"'
        target = spec.data_type.value

        if spec.pattern and spec.pattern.standardization_expr:
            # Apply standardization before cast
            expr = spec.pattern.standardization_expr.format(col=spec.column_name)
            selects.append(f"TRY_CAST({expr} AS {target}) AS {col}")
        else:
            selects.append(f"TRY_CAST({col} AS {target}) AS {col}")

    return (
        f'CREATE OR REPLACE TABLE "{typed_table}" AS SELECT {", ".join(selects)} FROM "{raw_table}"'
    )


def _generate_quarantine_sql(
    raw_table: str,
    quarantine_table: str,
    specs: list[ColumnTypeSpec],
) -> str:
    """Generate quarantine table for rows where any cast fails."""
    checks = []
    for spec in specs:
        col = f'"{spec.column_name}"'
        target = spec.data_type.value

        if spec.pattern and spec.pattern.standardization_expr:
            expr = spec.pattern.standardization_expr.format(col=spec.column_name)
            checks.append(f"(TRY_CAST({expr} AS {target}) IS NULL AND {col} IS NOT NULL)")
        else:
            checks.append(f"(TRY_CAST({col} AS {target}) IS NULL AND {col} IS NOT NULL)")

    where = " OR ".join(checks) if checks else "FALSE"
    return f'CREATE OR REPLACE TABLE "{quarantine_table}" AS SELECT *, CURRENT_TIMESTAMP AS _quarantined_at FROM "{raw_table}" WHERE {where}'


def resolve_types(
    table_id: str,
    duckdb_conn: duckdb.DuckDBPyConnection,
    session: Session,
    min_confidence: float = 0.85,
) -> Result[TypeResolutionResult]:
    """Resolve types for a raw table using DuckDB SQL.

    1. Load TypeCandidates (from inference)
    2. Select best candidate per column
    3. Generate and execute typed table SQL
    4. Generate and execute quarantine table SQL
    5. Return stats

    Args:
        table_id: ID of the raw table to resolve
        duckdb_conn: DuckDB connection
        session: SQLAlchemy session
        min_confidence: Minimum confidence threshold for automatic type selection

    Returns:
        Result containing TypeResolutionResult with table names and row counts
    """
    # Load table with columns and type candidates
    stmt = (
        select(Table)
        .where(Table.table_id == table_id)
        .options(
            selectinload(Table.columns).selectinload(Column.type_candidates),
            selectinload(Table.columns).selectinload(Column.type_decision),
        )
    )
    result = session.execute(stmt)
    table = result.scalar_one_or_none()

    if not table:
        return Result.fail(f"Table not found: {table_id}")
    if table.layer != "raw":
        return Result.fail(f"Table is not a raw table: {table.layer}")
    if not table.duckdb_path:
        return Result.fail(f"Table has no DuckDB path: {table_id}")

    raw_table = table.duckdb_path
    base_name = raw_table.replace("raw_", "")
    typed_table = f"typed_{base_name}"
    quarantine_table = f"quarantine_{base_name}"

    # Select best candidates
    specs = _select_best_candidates(table.columns, min_confidence)

    # Persist TypeDecision records for columns that don't already have one
    # (columns with pre-existing TypeDecision are human overrides, don't overwrite).
    # Use relationship assignment (column=raw_col) instead of FK assignment
    # (column_id=...) so that back_populates fires and raw_col.type_decision
    # is populated for the copy step below.
    raw_col_by_id = {col.column_id: col for col in table.columns}
    columns_with_decision = {col.column_id for col in table.columns if col.type_decision}
    for spec in specs:
        if spec.column_id not in columns_with_decision:
            raw_col = raw_col_by_id[spec.column_id]
            type_decision = TypeDecision(
                decision_id=str(uuid4()),
                column=raw_col,
                decided_type=spec.data_type.value,
                decision_source=spec.decision_source,
                decided_at=datetime.now(UTC),
                decision_reason=spec.decision_reason,
            )
            session.add(type_decision)

    # Generate and execute SQL
    try:
        typed_sql = _generate_typed_table_sql(raw_table, typed_table, specs)
        duckdb_conn.execute(typed_sql)

        quarantine_sql = _generate_quarantine_sql(raw_table, quarantine_table, specs)
        duckdb_conn.execute(quarantine_sql)
    except Exception as e:
        return Result.fail(f"SQL execution failed: {e}")

    # Get row counts
    total_result = duckdb_conn.execute(f'SELECT COUNT(*) FROM "{raw_table}"').fetchone()
    total_rows = total_result[0] if total_result else 0
    typed_result = duckdb_conn.execute(f'SELECT COUNT(*) FROM "{typed_table}"').fetchone()
    typed_rows = typed_result[0] if typed_result else 0
    quarantine_result = duckdb_conn.execute(f'SELECT COUNT(*) FROM "{quarantine_table}"').fetchone()
    quarantine_rows = quarantine_result[0] if quarantine_result else 0

    # Create metadata records for typed and quarantine tables
    typed_table_record = Table(
        table_id=str(uuid4()),
        source_id=table.source_id,
        table_name=table.table_name,
        layer="typed",
        duckdb_path=typed_table,
        row_count=typed_rows,
    )
    session.add(typed_table_record)

    quarantine_table_record = Table(
        table_id=str(uuid4()),
        source_id=table.source_id,
        table_name=table.table_name,
        layer="quarantine",
        duckdb_path=quarantine_table,
        row_count=quarantine_rows,
    )
    session.add(quarantine_table_record)

    # Create column records for typed table
    typed_column_map: dict[str, str] = {}  # column_name -> typed_column_id
    for i, spec in enumerate(specs):
        typed_col_id = str(uuid4())
        typed_col = Column(
            column_id=typed_col_id,
            table_id=typed_table_record.table_id,
            column_name=spec.column_name,
            column_position=i,
            raw_type="VARCHAR",
            resolved_type=spec.data_type.value,
        )
        session.add(typed_col)
        typed_column_map[spec.column_name] = typed_col_id

    # Create column records for quarantine table (all columns + _quarantined_at)
    for i, spec in enumerate(specs):
        quarantine_col = Column(
            column_id=str(uuid4()),
            table_id=quarantine_table_record.table_id,
            column_name=spec.column_name,
            column_position=i,
            raw_type="VARCHAR",
            resolved_type="VARCHAR",  # Quarantine keeps original VARCHAR
        )
        session.add(quarantine_col)

    # Add _quarantined_at column
    quarantine_meta_col = Column(
        column_id=str(uuid4()),
        table_id=quarantine_table_record.table_id,
        column_name="_quarantined_at",
        column_position=len(specs),
        raw_type="TIMESTAMP",
        resolved_type="TIMESTAMP",
    )
    session.add(quarantine_meta_col)

    # Copy TypeDecision and TypeCandidate from raw columns to typed columns.
    # Raw columns keep originals (audit trail); typed columns get copies so
    # downstream consumers can query by typed column_id directly.
    for raw_col in table.columns:
        target_col_id = typed_column_map.get(raw_col.column_name)
        if target_col_id is None:
            continue

        if raw_col.type_decision:
            td = raw_col.type_decision
            session.add(
                TypeDecision(
                    decision_id=str(uuid4()),
                    column_id=target_col_id,
                    decided_type=td.decided_type,
                    decision_source=td.decision_source,
                    decided_at=td.decided_at,
                    decided_by=td.decided_by,
                    previous_type=td.previous_type,
                    decision_reason=td.decision_reason,
                )
            )

        for tc in raw_col.type_candidates:
            session.add(
                TypeCandidate(
                    candidate_id=str(uuid4()),
                    column_id=target_col_id,
                    detected_at=tc.detected_at,
                    data_type=tc.data_type,
                    confidence=tc.confidence,
                    parse_success_rate=tc.parse_success_rate,
                    failed_examples=tc.failed_examples,
                    detected_pattern=tc.detected_pattern,
                    pattern_match_rate=tc.pattern_match_rate,
                    detected_unit=tc.detected_unit,
                    unit_confidence=tc.unit_confidence,
                )
            )

    # Build column results
    column_results = []
    for spec in specs:
        col = f'"{spec.column_name}"'
        target = spec.data_type.value

        if spec.pattern and spec.pattern.standardization_expr:
            expr = spec.pattern.standardization_expr.format(col=spec.column_name)
            cast_expr = f"TRY_CAST({expr} AS {target})"
        else:
            cast_expr = f"TRY_CAST({col} AS {target})"

        success_result = duckdb_conn.execute(
            f'SELECT COUNT(*) FROM "{raw_table}" WHERE {cast_expr} IS NOT NULL OR {col} IS NULL'
        ).fetchone()
        success = success_result[0] if success_result else 0
        failures = total_rows - success

        column_results.append(
            ColumnCastResult(
                column_id=spec.column_id,
                column_ref=ColumnRef(table_name=table.table_name, column_name=spec.column_name),
                source_type="VARCHAR",
                target_type=spec.data_type,
                success_count=success,
                failure_count=failures,
                success_rate=success / total_rows if total_rows > 0 else 1.0,
            )
        )

    return Result.ok(
        TypeResolutionResult(
            typed_table_id=typed_table_record.table_id,
            typed_table_name=typed_table,
            quarantine_table_name=quarantine_table,
            total_rows=total_rows,
            typed_rows=typed_rows,
            quarantined_rows=quarantine_rows,
            column_results=column_results,
        )
    )

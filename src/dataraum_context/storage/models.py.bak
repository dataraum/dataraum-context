"""SQLAlchemy models for metadata storage.

Note: This module defines database models (SQLAlchemy ORM).
For API/interface models (Pydantic), see core.models.
"""

from datetime import datetime
from typing import Any

from sqlalchemy import (
    JSON,
    Boolean,
    DateTime,
    Float,
    ForeignKey,
    Index,
    Integer,
    String,
    Text,
    UniqueConstraint,
)
from sqlalchemy.orm import Mapped, mapped_column, relationship

from dataraum_context.storage.base import Base, generate_uuid

# ============================================================================
# Schema Version Tracking
# ============================================================================


class SchemaVersion(Base):
    """Track schema versions for compatibility."""

    __tablename__ = "schema_version"

    version: Mapped[str] = mapped_column(String, primary_key=True)
    applied_at: Mapped[datetime] = mapped_column(DateTime, nullable=False, default=datetime.utcnow)


# ============================================================================
# Core Tables
# ============================================================================


class Source(Base):
    """Data sources (CSV files, databases, APIs, etc.)."""

    __tablename__ = "sources"

    source_id = Column(String, primary_key=True, default=generate_uuid)
    name = Column(String, nullable=False, unique=True)
    source_type = Column(String, nullable=False)  # 'csv', 'parquet', 'postgres', 'api'
    connection_config = Column(JSON)  # encrypted connection details
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    updated_at = Column(DateTime, nullable=False, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    tables = relationship("Table", back_populates="source", cascade="all, delete-orphan")


class Table(Base):
    """Tables from data sources (raw, typed, or quarantine layers)."""

    __tablename__ = "tables"
    __table_args__ = (
        UniqueConstraint("source_id", "table_name", "layer", name="uq_source_table_layer"),
    )
    __table_args__ = (
        UniqueConstraint("source_id", "table_name", "layer", name="uq_source_table_layer"),
    )

    table_id = Column(String, primary_key=True, default=generate_uuid)
    source_id = Column(String, ForeignKey("sources.source_id"), nullable=False)
    table_name = Column(String, nullable=False)
    layer = Column(String, nullable=False)  # 'raw', 'typed', 'quarantine'
    duckdb_path = Column(String)  # path to parquet/duckdb file
    row_count = Column(Integer)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    last_profiled_at = Column(DateTime)

    # Relationships
    source = relationship("Source", back_populates="tables")
    columns = relationship("Column", back_populates="table", cascade="all, delete-orphan")
    entity_detections = relationship(
        "TableEntity", back_populates="table", cascade="all, delete-orphan"
    )
    quality_rules = relationship(
        "QualityRule", back_populates="table", cascade="all, delete-orphan"
    )
    quality_scores = relationship(
        "QualityScore", back_populates="table", cascade="all, delete-orphan"
    )
    ontology_applications = relationship(
        "OntologyApplication", back_populates="table", cascade="all, delete-orphan"
    )


class Column(Base):
    """Columns in tables."""

    __tablename__ = "columns"
    __table_args__ = (UniqueConstraint("table_id", "column_name", name="uq_table_column"),)

    column_id = Column(String, primary_key=True, default=generate_uuid)
    table_id = Column(String, ForeignKey("tables.table_id", ondelete="CASCADE"), nullable=False)
    column_name = Column(String, nullable=False)
    column_position = Column(Integer, nullable=False)
    raw_type = Column(String)  # original inferred type
    resolved_type = Column(String)  # final decided type

    # Relationships
    table = relationship("Table", back_populates="columns")
    profiles = relationship("ColumnProfile", back_populates="column", cascade="all, delete-orphan")
    type_candidates = relationship(
        "TypeCandidate", back_populates="column", cascade="all, delete-orphan"
    )
    type_decision = relationship(
        "TypeDecision", back_populates="column", uselist=False, cascade="all, delete-orphan"
    )
    semantic_annotation = relationship(
        "SemanticAnnotation", back_populates="column", uselist=False, cascade="all, delete-orphan"
    )
    temporal_profile = relationship(
        "TemporalProfile", back_populates="column", uselist=False, cascade="all, delete-orphan"
    )
    quality_rules = relationship(
        "QualityRule", back_populates="column", cascade="all, delete-orphan"
    )
    quality_scores = relationship(
        "QualityScore", back_populates="column", cascade="all, delete-orphan"
    )
    relationships_from = relationship(
        "Relationship", foreign_keys="Relationship.from_column_id", back_populates="from_column"
    )
    relationships_to = relationship(
        "Relationship", foreign_keys="Relationship.to_column_id", back_populates="to_column"
    )


# Index for column lookups
Index("idx_columns_table", Column.table_id)


# ============================================================================
# Statistical Metadata Tables
# ============================================================================


class ColumnProfile(Base):
    """Statistical profile of a column (versioned by profiled_at)."""

    __tablename__ = "column_profiles"

    profile_id = Column(String, primary_key=True, default=generate_uuid)
    column_id = Column(String, ForeignKey("columns.column_id", ondelete="CASCADE"), nullable=False)
    profiled_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Counts
    total_count = Column(Integer, nullable=False)
    null_count = Column(Integer, nullable=False)
    distinct_count = Column(Integer)

    # Numeric stats
    min_value = Column(Float)
    max_value = Column(Float)
    mean_value = Column(Float)
    stddev_value = Column(Float)
    percentiles = Column(JSON)  # {p25, p50, p75, p95, p99}

    # String stats
    min_length = Column(Integer)
    max_length = Column(Integer)
    avg_length = Column(Float)

    # Distribution
    histogram = Column(JSON)  # [{bucket_min, bucket_max, count}, ...]
    top_values = Column(JSON)  # [{value, count, percentage}, ...]

    # Computed metrics
    cardinality_ratio = Column(Float)  # distinct/total
    null_ratio = Column(Float)  # null/total

    # Relationships
    column = relationship("Column", back_populates="profiles")


# Index for finding latest profile
Index("idx_column_profiles_latest", ColumnProfile.column_id, ColumnProfile.profiled_at.desc())


class TypeCandidate(Base):
    """Type candidates from pattern detection."""

    __tablename__ = "type_candidates"

    candidate_id = Column(String, primary_key=True, default=generate_uuid)
    column_id = Column(String, ForeignKey("columns.column_id", ondelete="CASCADE"), nullable=False)
    detected_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    data_type = Column(String, nullable=False)  # 'INTEGER', 'DOUBLE', 'DATE', etc.
    confidence = Column(Float, nullable=False)
    parse_success_rate = Column(Float)
    failed_examples = Column(JSON)  # sample of unparseable values

    # Pattern info
    detected_pattern = Column(String)  # 'iso_date', 'uuid', 'email', etc.
    pattern_match_rate = Column(Float)

    # Unit detection (from Pint)
    detected_unit = Column(String)  # 'kg', 'm/s', 'USD', etc.
    unit_confidence = Column(Float)

    # Relationships
    column = relationship("Column", back_populates="type_candidates")


Index("idx_type_candidates_column", TypeCandidate.column_id)


class TypeDecision(Base):
    """Type decisions (human-reviewable)."""

    __tablename__ = "type_decisions"
    __table_args__ = (UniqueConstraint("column_id", name="uq_column_type_decision"),)

    decision_id = Column(String, primary_key=True, default=generate_uuid)
    column_id = Column(String, ForeignKey("columns.column_id", ondelete="CASCADE"), nullable=False)

    decided_type = Column(String, nullable=False)
    decision_source = Column(String, nullable=False)  # 'auto', 'manual', 'rule'
    decided_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    decided_by = Column(String)  # 'system' or user identifier

    # Audit trail
    previous_type = Column(String)
    decision_reason = Column(String)

    # Relationships
    column = relationship("Column", back_populates="type_decision")


# ============================================================================
# Semantic Metadata Tables
# ============================================================================


class SemanticAnnotation(Base):
    """Semantic annotations (LLM-generated or manual)."""

    __tablename__ = "semantic_annotations"
    __table_args__ = (UniqueConstraint("column_id", name="uq_column_semantic_annotation"),)

    annotation_id = Column(String, primary_key=True, default=generate_uuid)
    column_id = Column(String, ForeignKey("columns.column_id", ondelete="CASCADE"), nullable=False)

    # Classification
    semantic_role = Column(
        String
    )  # 'measure', 'dimension', 'key', 'foreign_key', 'attribute', 'timestamp'
    entity_type = Column(String)  # 'customer', 'transaction', 'product', etc.

    # Business terms
    business_name = Column(String)
    business_description = Column(Text)
    business_domain = Column(String)  # 'finance', 'marketing', 'operations'

    # Ontology mapping
    ontology_term = Column(String)
    ontology_uri = Column(String)

    # Provenance
    annotation_source = Column(String)  # 'llm', 'manual', 'override'
    annotated_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    annotated_by = Column(String)  # model name or user ID
    confidence = Column(Float)

    # Relationships
    column = relationship("Column", back_populates="semantic_annotation")


class TableEntity(Base):
    """Entity detection at table level."""

    __tablename__ = "table_entities"

    entity_id = Column(String, primary_key=True, default=generate_uuid)
    table_id = Column(String, ForeignKey("tables.table_id", ondelete="CASCADE"), nullable=False)

    detected_entity_type = Column(String, nullable=False)
    description = Column(Text)  # LLM-generated table description
    confidence = Column(Float)
    evidence = Column(JSON)  # columns/patterns that led to detection

    # Grain
    grain_columns = Column(JSON)  # list of column names
    is_fact_table = Column(Boolean)
    is_dimension_table = Column(Boolean)

    # Provenance
    detection_source = Column(String)  # 'llm', 'manual', 'override'
    detected_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Relationships
    table = relationship("Table", back_populates="entity_detections")


# ============================================================================
# Topological Metadata Tables
# ============================================================================


class Relationship(Base):
    """Detected relationships (from TDA or other methods)."""

    __tablename__ = "relationships"
    __table_args__ = (
        UniqueConstraint("from_column_id", "to_column_id", name="uq_relationship_columns"),
    )

    relationship_id = Column(String, primary_key=True, default=generate_uuid)

    # Source side
    from_table_id = Column(String, ForeignKey("tables.table_id"), nullable=False)
    from_column_id = Column(String, ForeignKey("columns.column_id"), nullable=False)

    # Target side
    to_table_id = Column(String, ForeignKey("tables.table_id"), nullable=False)
    to_column_id = Column(String, ForeignKey("columns.column_id"), nullable=False)

    # Classification
    relationship_type = Column(
        String, nullable=False
    )  # 'foreign_key', 'hierarchy', 'correlation', 'semantic'
    cardinality = Column(String)  # '1:1', '1:n', 'n:m'

    # Confidence
    confidence = Column(Float, nullable=False)
    detection_method = Column(String)  # 'tda', 'value_overlap', 'name_similarity'
    evidence = Column(JSON)  # TDA features, overlap stats

    # Verification
    is_confirmed = Column(Boolean, default=False)
    confirmed_at = Column(DateTime)
    confirmed_by = Column(String)

    detected_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Relationships
    from_column = relationship(
        "Column", foreign_keys=[from_column_id], back_populates="relationships_from"
    )
    to_column = relationship(
        "Column", foreign_keys=[to_column_id], back_populates="relationships_to"
    )


Index("idx_relationships_from", Relationship.from_table_id)
Index("idx_relationships_to", Relationship.to_table_id)


class JoinPath(Base):
    """Computed join paths between tables."""

    __tablename__ = "join_paths"
    __table_args__ = (
        UniqueConstraint("from_table_id", "to_table_id", "path_steps", name="uq_join_path"),
    )

    path_id = Column(String, primary_key=True, default=generate_uuid)

    from_table_id = Column(String, ForeignKey("tables.table_id"), nullable=False)
    to_table_id = Column(String, ForeignKey("tables.table_id"), nullable=False)

    path_steps = Column(JSON, nullable=False)  # [{from_col, to_table, to_col}, ...]
    path_length = Column(Integer, nullable=False)
    total_confidence = Column(Float)

    computed_at = Column(DateTime, nullable=False, default=datetime.utcnow)


# ============================================================================
# Temporal Metadata Tables
# ============================================================================


class TemporalProfile(Base):
    """Temporal profiles for time columns."""

    __tablename__ = "temporal_profiles"
    __table_args__ = (UniqueConstraint("column_id", name="uq_column_temporal_profile"),)

    temporal_id = Column(String, primary_key=True, default=generate_uuid)
    column_id = Column(String, ForeignKey("columns.column_id", ondelete="CASCADE"), nullable=False)

    # Range
    min_timestamp = Column(DateTime)
    max_timestamp = Column(DateTime)

    # Granularity
    detected_granularity = Column(
        String
    )  # 'second', 'minute', 'hour', 'day', 'week', 'month', 'quarter', 'year'
    granularity_confidence = Column(Float)
    dominant_gap = Column(String)  # stored as ISO duration string

    # Completeness
    expected_periods = Column(Integer)
    actual_periods = Column(Integer)
    completeness_ratio = Column(Float)

    # Gaps
    gap_count = Column(Integer)
    largest_gap = Column(String)  # stored as ISO duration string
    gap_details = Column(JSON)  # [{start, end, missing_periods}, ...]

    # Patterns
    has_seasonality = Column(Boolean)
    seasonality_period = Column(String)
    trend_direction = Column(String)  # 'increasing', 'decreasing', 'stable'

    profiled_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Relationships
    column = relationship("Column", back_populates="temporal_profile")


# ============================================================================
# Quality Metadata Tables
# ============================================================================


class QualityRule(Base):
    """Quality rules (LLM-generated or manual)."""

    __tablename__ = "quality_rules"

    rule_id = Column(String, primary_key=True, default=generate_uuid)

    # Scope
    table_id = Column(String, ForeignKey("tables.table_id"), nullable=False)
    column_id = Column(String, ForeignKey("columns.column_id"))  # NULL for table-level

    # Rule definition
    rule_name = Column(String, nullable=False)
    rule_type = Column(
        String, nullable=False
    )  # 'not_null', 'unique', 'range', 'pattern', 'referential', 'custom'
    rule_expression = Column(Text)  # SQL expression (DuckDB syntax)
    rule_parameters = Column(JSON)  # {min, max, pattern, etc.}

    # Metadata
    severity = Column(String, default="warning")  # 'error', 'warning', 'info'
    source = Column(String, nullable=False)  # 'llm', 'ontology', 'default', 'manual'
    description = Column(Text)  # LLM-generated rationale

    # Status
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    created_by = Column(String)

    # Relationships
    table = relationship("Table", back_populates="quality_rules")
    column = relationship("Column", back_populates="quality_rules")
    results = relationship("QualityResult", back_populates="rule", cascade="all, delete-orphan")


Index("idx_quality_rules_table", QualityRule.table_id)
Index("idx_quality_rules_column", QualityRule.column_id)


class QualityResult(Base):
    """Quality rule execution results."""

    __tablename__ = "quality_results"

    result_id = Column(String, primary_key=True, default=generate_uuid)
    rule_id = Column(
        String, ForeignKey("quality_rules.rule_id", ondelete="CASCADE"), nullable=False
    )

    executed_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Results
    total_records = Column(Integer)
    passed_records = Column(Integer)
    failed_records = Column(Integer)
    pass_rate = Column(Float)

    # Failure details
    failure_samples = Column(JSON)  # sample of failing records

    # Trend
    previous_pass_rate = Column(Float)
    trend_direction = Column(String)  # 'improving', 'degrading', 'stable'

    # Relationships
    rule = relationship("QualityRule", back_populates="results")


class QualityScore(Base):
    """Aggregate quality scores."""

    __tablename__ = "quality_scores"

    score_id = Column(String, primary_key=True, default=generate_uuid)

    # Scope (one of these is set)
    table_id = Column(String, ForeignKey("tables.table_id"))
    column_id = Column(String, ForeignKey("columns.column_id"))

    # Scores by dimension (0-1)
    completeness_score = Column(Float)
    validity_score = Column(Float)
    consistency_score = Column(Float)
    uniqueness_score = Column(Float)
    timeliness_score = Column(Float)

    # Overall
    overall_score = Column(Float)

    computed_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Relationships
    table = relationship("Table", back_populates="quality_scores")
    column = relationship("Column", back_populates="quality_scores")


# ============================================================================
# Ontology Definitions
# ============================================================================


class Ontology(Base):
    """Ontological contexts."""

    __tablename__ = "ontologies"

    ontology_id = Column(String, primary_key=True, default=generate_uuid)

    name = Column(String, nullable=False, unique=True)
    description = Column(Text)
    version = Column(String)

    # Content
    concepts = Column(JSON)  # [{name, indicators, temporal_behavior}, ...]
    metrics = Column(JSON)  # [{name, formula, required_concepts}, ...]
    quality_rules = Column(JSON)  # default rules for this ontology
    semantic_hints = Column(JSON)  # column name patterns

    # Metadata
    is_builtin = Column(Boolean, default=False)
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    updated_at = Column(DateTime, nullable=False, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Relationships
    applications = relationship(
        "OntologyApplication", back_populates="ontology", cascade="all, delete-orphan"
    )


class OntologyApplication(Base):
    """Ontology application log."""

    __tablename__ = "ontology_applications"

    application_id = Column(String, primary_key=True, default=generate_uuid)

    table_id = Column(String, ForeignKey("tables.table_id"), nullable=False)
    ontology_id = Column(String, ForeignKey("ontologies.ontology_id"), nullable=False)

    # Results
    matched_concepts = Column(JSON)  # which concepts were found
    applicable_metrics = Column(JSON)  # which metrics can be computed
    applied_rules = Column(JSON)  # which quality rules were added

    applied_at = Column(DateTime, nullable=False, default=datetime.utcnow)

    # Relationships
    table = relationship("Table", back_populates="ontology_applications")
    ontology = relationship("Ontology", back_populates="applications")


# ============================================================================
# Dataflow Checkpoints (Human-in-Loop)
# ============================================================================


class Checkpoint(Base):
    """Dataflow execution checkpoints for resume capability."""

    __tablename__ = "checkpoints"

    checkpoint_id = Column(String, primary_key=True, default=generate_uuid)

    dataflow_name = Column(String, nullable=False)
    source_id = Column(String, ForeignKey("sources.source_id"), nullable=False)

    # Status
    status = Column(String, nullable=False)  # 'pending_review', 'approved', 'completed', 'failed'
    checkpoint_type = Column(
        String, nullable=False
    )  # 'type_review', 'quarantine_review', 'semantic_review'

    # Timing
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    resumed_at = Column(DateTime)
    completed_at = Column(DateTime)

    # State for resume (Hamilton inputs/outputs)
    checkpoint_state = Column(JSON)  # serialized intermediate results

    # Results
    result_summary = Column(JSON)
    error_message = Column(Text)

    # Relationships
    review_items = relationship(
        "ReviewQueue", back_populates="checkpoint", cascade="all, delete-orphan"
    )


Index("idx_checkpoints_status", Checkpoint.status)


class ReviewQueue(Base):
    """Human review queue."""

    __tablename__ = "review_queue"

    review_id = Column(String, primary_key=True, default=generate_uuid)
    checkpoint_id = Column(String, ForeignKey("checkpoints.checkpoint_id"), nullable=False)

    review_type = Column(
        String, nullable=False
    )  # 'type_decision', 'quarantine', 'semantic', 'relationship'
    item_id = Column(String, nullable=False)  # reference to the item needing review

    # Context
    context_data = Column(JSON)  # relevant info for reviewer
    suggested_action = Column(JSON)

    # Status
    status = Column(String, default="pending")  # 'pending', 'approved', 'rejected', 'modified'
    reviewed_at = Column(DateTime)
    reviewed_by = Column(String)
    review_notes = Column(Text)

    # Relationships
    checkpoint = relationship("Checkpoint", back_populates="review_items")


Index(
    "idx_review_queue_status",
    ReviewQueue.status,
    postgresql_where=(ReviewQueue.status == "pending"),
)


# ============================================================================
# LLM Response Cache
# ============================================================================


class LLMCache(Base):
    """Cache LLM responses to avoid redundant API calls."""

    __tablename__ = "llm_cache"

    cache_id = Column(String, primary_key=True, default=generate_uuid)

    # Cache key (hash of inputs)
    cache_key = Column(String, nullable=False, unique=True)
    feature = Column(
        String, nullable=False
    )  # 'semantic_analysis', 'quality_rules', 'suggested_queries', 'context_summary'

    # Request context
    source_id = Column(String, ForeignKey("sources.source_id"))
    table_ids = Column(JSON)  # list of table IDs included
    ontology = Column(String)

    # LLM details
    provider = Column(String, nullable=False)  # 'anthropic', 'openai', 'local'
    model = Column(String, nullable=False)
    prompt_hash = Column(String)  # hash of prompt template used

    # Response
    response_json = Column(JSON, nullable=False)
    input_tokens = Column(Integer)
    output_tokens = Column(Integer)

    # Timing
    created_at = Column(DateTime, nullable=False, default=datetime.utcnow)
    expires_at = Column(DateTime)  # based on cache_ttl_seconds config

    # Invalidation
    is_valid = Column(Boolean, default=True)


Index("idx_llm_cache_key", LLMCache.cache_key)
Index(
    "idx_llm_cache_feature",
    LLMCache.feature,
    LLMCache.source_id,
    postgresql_where=(LLMCache.is_valid == True),
)

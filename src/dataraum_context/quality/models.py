"""Quality layer models.

Defines data structures for quality analysis results and issues.

This module contains models for:
- Quality issues (unified format for all pillars)
- Temporal quality analysis (seasonality, trends, change points)
- Topological quality analysis (Betti numbers, persistence, cycles)
- Quality dimension and severity enums (for classification, not scoring)

Note: Scoring models have been removed. This module focuses on raw analysis
results and issue detection, not computed quality scores.
"""

from __future__ import annotations

from datetime import datetime
from enum import Enum
from typing import Any

from pydantic import BaseModel, Field

from dataraum_context.core.models.base import ColumnRef, DecisionSource, QualitySeverity

# === Statistical Quality Models ===
# These models represent quality assessment results (Benford's Law, outlier detection)


class BenfordAnalysis(BaseModel):
    """Benford's Law compliance analysis."""

    chi_square: float
    p_value: float
    is_compliant: bool  # p_value > 0.05
    digit_distribution: dict[str, float]  # {1: 0.301, 2: 0.176, ...}
    interpretation: str


class OutlierDetection(BaseModel):
    """Outlier detection results."""

    # IQR Method
    iqr_lower_fence: float
    iqr_upper_fence: float
    iqr_outlier_count: int
    iqr_outlier_ratio: float

    # Isolation Forest
    isolation_forest_score: float  # Average anomaly score
    isolation_forest_anomaly_count: int
    isolation_forest_anomaly_ratio: float

    # Sample outliers
    outlier_samples: list[dict[str, Any]] = Field(default_factory=list)  # [{value, method, score}]


class StatisticalQualityResult(BaseModel):
    """Comprehensive statistical quality assessment.

    This is the Pydantic source of truth for statistical quality metrics.
    Gets serialized to StatisticalQualityMetrics.quality_data JSONB field.
    """

    column_id: str
    column_ref: ColumnRef

    # Benford's Law (for financial/count columns)
    benford_analysis: BenfordAnalysis | None = None

    # Outlier detection
    outlier_detection: OutlierDetection | None = None

    # Quality issues detected
    quality_issues: list[dict[str, Any]] = Field(
        default_factory=list
    )  # [{issue_type, severity, description}]


# === Quality Enums ===


class QualityDimension(str, Enum):
    """Standard data quality dimensions.

    Used for classifying quality issues by dimension, NOT for computing scores.
    """

    COMPLETENESS = "completeness"
    VALIDITY = "validity"
    CONSISTENCY = "consistency"
    UNIQUENESS = "uniqueness"
    TIMELINESS = "timeliness"
    ACCURACY = "accuracy"


class QualitySynthesisSeverity(str, Enum):
    """Severity levels for quality issues."""

    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


# === Quality Issue Models ===


class QualityIssue(BaseModel):
    """A quality issue detected during analysis.

    This is the simple issue model used by temporal/topological modules.
    """

    issue_type: str  # 'low_completeness', 'large_gap', 'stale_data', 'irregular_updates'
    severity: str  # 'critical', 'warning', 'info'
    description: str
    evidence: dict[str, Any] = Field(default_factory=dict)
    detected_at: datetime | None = None


class QualitySynthesisIssue(BaseModel):
    """A comprehensive quality issue detected during synthesis.

    This is the full-featured issue model used by the quality synthesis layer.
    Includes source tracking, dimension classification, and recommendations.
    """

    issue_id: str = Field(description="Unique identifier for this issue")
    issue_type: str = Field(description="Type of issue (e.g., 'benford_violation', 'large_gap')")
    severity: QualitySynthesisSeverity = Field(description="Severity level")
    dimension: QualityDimension = Field(description="Which quality dimension this affects")

    # Scope
    table_id: str | None = Field(None, description="Table ID if table-level issue")
    column_id: str | None = Field(None, description="Column ID if column-level issue")
    column_name: str | None = Field(None, description="Column name for readability")

    # Description
    description: str = Field(description="Human-readable description of the issue")
    recommendation: str | None = Field(None, description="Suggested remediation")

    # Evidence
    evidence: dict[str, Any] = Field(
        default_factory=dict, description="Supporting data (metrics, examples, etc.)"
    )

    # Source tracking
    source_pillar: int = Field(description="Which pillar detected this (1-5)")
    source_module: str = Field(description="Which module detected this")
    detected_at: datetime = Field(description="When this was detected")


class QualityRule(BaseModel):
    """A quality rule for validation.

    Rules are generated by LLM or loaded from configuration.
    They define conditions that data must satisfy.
    """

    rule_id: str = Field(description="Unique identifier for this rule")
    table_name: str = Field(description="Table this rule applies to")
    column_name: str | None = Field(
        None, description="Column this rule applies to (if column-level)"
    )
    rule_name: str = Field(description="Human-readable rule name")
    rule_type: str = Field(description="Type of rule (e.g., 'not_null', 'range', 'pattern')")
    rule_expression: str = Field(description="SQL expression for the rule")
    parameters: dict[str, Any] = Field(default_factory=dict, description="Rule parameters")
    severity: QualitySeverity = Field(default=QualitySeverity.WARNING, description="Severity level")
    source: DecisionSource = Field(default=DecisionSource.AUTO, description="How rule was created")
    description: str | None = Field(None, description="Human-readable description")


# === Temporal Quality Models ===


class SeasonalDecompositionResult(BaseModel):
    """Seasonal decomposition results (additive or multiplicative)."""

    seasonal_component: list[float] = Field(default_factory=list)
    trend_component: list[float] = Field(default_factory=list)
    residual_component: list[float] = Field(default_factory=list)
    model_type: str = "additive"  # 'additive' or 'multiplicative'

    # Strength metrics
    seasonality_strength: float | None = None  # 1 - Var(resid) / Var(detrended)
    trend_strength: float | None = None  # 1 - Var(resid) / Var(deseasonalized)
    seasonal_pattern_summary: dict[str, Any] | None = None
    period: int | None = None


class SeasonalityAnalysis(BaseModel):
    """Seasonality detection results."""

    has_seasonality: bool
    strength: float  # 0-1, how strong the seasonal component is
    period: str | None = None  # 'daily', 'weekly', 'monthly', 'quarterly', 'yearly'
    period_length: int | None = None  # Number of observations in a period
    peaks: dict[str, int | float] = Field(
        default_factory=dict
    )  # e.g., {"month": 12, "day_of_week": 5}
    model_type: str | None = None  # 'additive' or 'multiplicative'
    decomposition: SeasonalDecompositionResult | None = None


class TrendAnalysis(BaseModel):
    """Trend detection results."""

    has_trend: bool
    strength: float  # 0-1
    direction: str  # 'increasing', 'decreasing', 'stable'
    slope: float | None = None
    autocorrelation_lag1: float | None = None


class ChangePointResult(BaseModel):
    """A detected change point in the time series."""

    change_point_id: str
    detected_at: datetime
    index_position: int | None = None

    # Change characteristics
    change_type: str  # 'trend_break', 'level_shift', 'variance_change'
    magnitude: float | None = None
    confidence: float  # 0-1

    # Before/after statistics
    mean_before: float | None = None
    mean_after: float | None = None
    variance_before: float | None = None
    variance_after: float | None = None
    detection_method: str  # 'pelt', 'cusum', etc.


class UpdateFrequencyAnalysis(BaseModel):
    """Update frequency and regularity analysis."""

    update_frequency_score: float  # 0-1
    median_interval_seconds: float
    interval_std: float | None = None
    interval_cv: float | None = None

    # Freshness
    last_update: datetime | None = None
    data_freshness_days: float | None = None
    is_stale: bool = False


class FiscalCalendarAnalysis(BaseModel):
    """Fiscal calendar alignment analysis."""

    fiscal_alignment_detected: bool
    fiscal_year_end_month: int | None = None  # 1-12
    confidence: float = 0.0  # 0-1

    # Period-end effects
    has_period_end_effects: bool = False
    period_end_spike_ratio: float | None = None
    detected_periods: list[str] = Field(default_factory=list)


class DistributionShiftResult(BaseModel):
    """Distribution shift detected between two periods."""

    shift_id: str

    # Time periods
    period1_start: datetime
    period1_end: datetime
    period2_start: datetime
    period2_end: datetime

    # Test results
    test_statistic: float  # KS statistic
    p_value: float
    is_significant: bool

    # Distribution characteristics
    period1_mean: float | None = None
    period2_mean: float | None = None
    period1_std: float | None = None
    period2_std: float | None = None

    # Interpretation
    shift_direction: str | None = None
    shift_magnitude: float | None = None


class DistributionStabilityAnalysis(BaseModel):
    """Distribution stability across time periods."""

    stability_score: float  # 0-1
    shift_count: int
    shifts: list[DistributionShiftResult] = Field(default_factory=list)

    # Overall statistics
    mean_ks_statistic: float | None = None
    max_ks_statistic: float | None = None


class TemporalGapInfo(BaseModel):
    """Information about a gap in the time series."""

    gap_start: datetime
    gap_end: datetime
    gap_length_days: float
    missing_periods: int
    severity: str  # 'minor', 'moderate', 'severe'


class TemporalCompletenessAnalysis(BaseModel):
    """Temporal completeness analysis."""

    completeness_ratio: float  # 0-1
    expected_periods: int
    actual_periods: int
    gap_count: int
    largest_gap_days: float | None = None
    gaps: list[TemporalGapInfo] = Field(default_factory=list)


class TemporalQualityResult(BaseModel):
    """Complete temporal quality analysis result.

    This is the Pydantic source of truth for temporal quality metrics.
    Gets serialized to TemporalQualityMetrics.temporal_data JSONB field.
    """

    metric_id: str
    column_id: str
    column_ref: ColumnRef
    column_name: str
    table_name: str
    computed_at: datetime

    # Basic temporal info
    min_timestamp: datetime
    max_timestamp: datetime
    span_days: float
    detected_granularity: str
    granularity_confidence: float

    # Seasonality
    seasonality: SeasonalityAnalysis | None = None

    # Trend
    trend: TrendAnalysis | None = None
    change_points: list[ChangePointResult] = Field(default_factory=list)

    # Update frequency
    update_frequency: UpdateFrequencyAnalysis | None = None

    # Fiscal calendar
    fiscal_calendar: FiscalCalendarAnalysis | None = None

    # Distribution stability
    distribution_stability: DistributionStabilityAnalysis | None = None

    # Completeness
    completeness: TemporalCompletenessAnalysis | None = None

    # Quality issues
    quality_issues: list[QualityIssue] = Field(default_factory=list)
    has_issues: bool = False


class TemporalTableSummary(BaseModel):
    """Table-level summary of temporal quality across multiple temporal columns."""

    table_id: str
    table_name: str
    temporal_column_count: int
    total_issues: int

    # Counts of columns with specific patterns
    columns_with_seasonality: int = 0
    columns_with_trends: int = 0
    columns_with_change_points: int = 0
    columns_with_fiscal_alignment: int = 0

    # Overall freshness
    stalest_column_days: int | None = None
    has_stale_columns: bool = False


# === Topological Quality Models ===


class BettiNumbers(BaseModel):
    """Betti numbers from homology analysis."""

    betti_0: int  # Connected components
    betti_1: int  # Cycles / holes
    betti_2: int  # Voids / cavities
    total_complexity: int  # Sum of Betti numbers
    is_connected: bool  # betti_0 == 1
    has_cycles: bool  # betti_1 > 0


class PersistencePoint(BaseModel):
    """A point in a persistence diagram."""

    dimension: int  # 0, 1, or 2
    birth: float
    death: float
    persistence: float  # death - birth


class PersistenceDiagram(BaseModel):
    """Persistence diagram for a specific dimension."""

    dimension: int
    points: list[PersistencePoint]
    max_persistence: float
    num_features: int
    persistent_entropy: float | None = None


class CycleDetection(BaseModel):
    """Detected persistent cycle."""

    cycle_id: str
    dimension: int
    birth: float
    death: float
    persistence: float
    involved_columns: list[str] = Field(default_factory=list)
    cycle_type: str | None = None  # 'money_flow', 'order_fulfillment', etc.
    is_anomalous: bool = False
    anomaly_reason: str | None = None
    first_detected: datetime
    last_seen: datetime


class StabilityAnalysis(BaseModel):
    """Homological stability assessment."""

    bottleneck_distance: float
    is_stable: bool
    stability_threshold: float = 0.1
    stability_level: str  # 'stable', 'minor_changes', 'significant_changes', 'unstable'

    # Change counts
    components_added: int = 0
    components_removed: int = 0
    cycles_added: int = 0
    cycles_removed: int = 0


class TopologicalAnomaly(BaseModel):
    """Detected topological anomaly."""

    anomaly_type: str  # 'unexpected_cycle', 'orphaned_component', 'complexity_spike'
    severity: str  # 'low', 'medium', 'high'
    description: str
    evidence: dict[str, Any] = Field(default_factory=dict)
    affected_tables: list[str] = Field(default_factory=list)
    affected_columns: list[str] = Field(default_factory=list)


class TopologicalQualityResult(BaseModel):
    """Comprehensive topological quality assessment.

    This is the Pydantic source of truth for topological quality metrics.
    Gets serialized to TopologicalQualityMetrics.topology_data JSONB field.
    """

    table_id: str
    table_name: str

    # Betti numbers
    betti_numbers: BettiNumbers

    # Persistence diagrams
    persistence_diagrams: list[PersistenceDiagram] = Field(default_factory=list)

    # Detected cycles
    persistent_cycles: list[CycleDetection] = Field(default_factory=list)

    # Stability
    stability: StabilityAnalysis | None = None

    # Complexity metrics
    structural_complexity: int
    persistent_entropy: float | None = None
    orphaned_components: int
    complexity_trend: str | None = None
    complexity_within_bounds: bool = True

    # Historical complexity context
    complexity_mean: float | None = None
    complexity_std: float | None = None
    complexity_z_score: float | None = None

    # Quality assessment
    has_anomalies: bool = False
    anomalies: list[TopologicalAnomaly] = Field(default_factory=list)
    anomalous_cycles: list[CycleDetection] = Field(default_factory=list)
    quality_warnings: list[str] = Field(default_factory=list)
    topology_description: str | None = None


# === Context-Focused Output Models ===
# These models are optimized for LLM/API consumption, surfacing raw metrics
# and issues with context - NOT scores.


class ColumnQualityContext(BaseModel):
    """Quality context for a single column - optimized for LLM consumption.

    Provides raw metrics and issues rather than computed scores.
    Consumers interpret the data within their own context.
    """

    column_id: str
    column_name: str
    table_id: str
    table_name: str

    # Raw metrics (not scores)
    null_ratio: float | None = None
    cardinality_ratio: float | None = None
    outlier_ratio: float | None = None
    parse_success_rate: float | None = None

    # Temporal metrics
    is_stale: bool | None = None
    data_freshness_days: float | None = None
    has_seasonality: bool | None = None
    has_trend: bool | None = None
    detected_granularity: str | None = Field(
        None, description="Detected time granularity (daily, weekly, monthly, etc.)"
    )
    completeness_ratio: float | None = Field(
        None, description="Ratio of actual to expected time periods (0-1)"
    )

    # Statistical metrics
    benford_compliant: bool | None = None

    # Semantic context (from LLM or manual annotation)
    semantic_role: str | None = Field(
        None, description="Role: identifier, measure, attribute, dimension"
    )
    entity_type: str | None = Field(
        None, description="Entity type: customer, product, transaction, etc."
    )
    business_name: str | None = Field(None, description="Human-friendly business name")

    # Derived column info (if this column is computed from others)
    derived_from: list[dict[str, Any]] | None = Field(
        None,
        description="List of derivation relationships: derivation_type, formula, match_rate, source_columns",
    )

    # Actionable flags for quick filtering decisions
    flags: list[str] = Field(
        default_factory=list,
        description="Flags like 'high_nulls', 'benford_violation', 'stale_data'",
    )

    # Issues with evidence (the core data)
    issues: list[QualitySynthesisIssue] = Field(default_factory=list)

    # Filter hints (LLM-generated when available)
    filter_hints: list[str] = Field(
        default_factory=list,
        description="Suggested SQL WHERE clauses for this column",
    )


class TableQualityContext(BaseModel):
    """Quality context for a table - optimized for LLM consumption.

    Groups column contexts and adds table-level issues.
    """

    table_id: str
    table_name: str
    row_count: int | None = None
    column_count: int = 0

    # Column contexts
    columns: list[ColumnQualityContext] = Field(default_factory=list)

    # Table-level issues (topological, domain)
    issues: list[QualitySynthesisIssue] = Field(default_factory=list)

    # Topological summary
    betti_0: int | None = None  # Connected components
    betti_1: int | None = None  # Cycles
    orphaned_components: int | None = None

    # Semantic context (from LLM or manual annotation)
    detected_entity_type: str | None = Field(
        None, description="Entity type: customer, order, product, etc."
    )
    is_fact_table: bool | None = Field(None, description="Whether this is a fact table")
    is_dimension_table: bool | None = Field(None, description="Whether this is a dimension table")

    # Domain analysis summary (if applicable)
    domain_anomaly_count: int = 0
    fiscal_stability: dict[str, Any] | None = None

    # Multicollinearity assessment (formatted for LLM)
    multicollinearity: dict[str, Any] | None = Field(
        None, description="LLM-formatted multicollinearity analysis from context_formatting"
    )

    # Relationships with issues
    problematic_relationships: list[dict[str, Any]] = Field(default_factory=list)

    # Flags for quick assessment
    flags: list[str] = Field(
        default_factory=list,
        description="Table-level flags like 'fragmented', 'has_anomalous_cycles'",
    )


class RelationshipContext(BaseModel):
    """Simplified relationship for LLM context output.

    Excludes internal IDs, focuses on human-readable names and key attributes.
    """

    from_table: str = Field(description="Source table name")
    from_column: str = Field(description="Source column name")
    to_table: str = Field(description="Target table name")
    to_column: str = Field(description="Target column name")
    relationship_type: str = Field(
        description="Type: foreign_key, semantic, correlation, hierarchy"
    )
    cardinality: str | None = Field(None, description="Cardinality: one_to_one, one_to_many, etc.")
    confidence: float = Field(description="Detection confidence (0-1)")
    detection_method: str = Field(description="How relationship was detected")


class DatasetQualityContext(BaseModel):
    """Quality context for an entire dataset - optimized for LLM consumption.

    Aggregates table contexts and cross-table issues.
    """

    # Tables
    tables: list[TableQualityContext] = Field(default_factory=list)

    # Detected relationships between tables
    relationships: list[RelationshipContext] = Field(
        default_factory=list,
        description="Detected relationships between tables with confidence scores",
    )

    # Cross-table issues
    cross_table_issues: list[QualitySynthesisIssue] = Field(default_factory=list)

    # Cross-table metrics
    cross_table_multicollinearity_severity: str | None = None
    cross_table_correlation_count: int = 0

    # Summary counts
    total_tables: int = 0
    total_columns: int = 0
    total_issues: int = 0
    critical_issue_count: int = 0

    # Issues by category (for quick overview)
    issues_by_severity: dict[str, int] = Field(default_factory=dict)
    issues_by_dimension: dict[str, int] = Field(default_factory=dict)

    # LLM-generated content (when available)
    summary: str | None = Field(None, description="Natural language summary of quality state")
    filter_recommendations: list[dict[str, Any]] = Field(
        default_factory=list,
        description="LLM-generated filter WHERE clauses with explanations",
    )

    # Metadata
    computed_at: datetime | None = None

"""Entropy Interpretation Agent - LLM-powered entropy analysis.

This agent interprets entropy metrics to generate contextual assumptions,
resolution actions, and explanations. It replaces hardcoded heuristics
with LLM-powered semantic interpretation.

See BACKLOG.md Step 2.5.3 for context.
"""

from __future__ import annotations

import json
import re
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

from sqlalchemy.ext.asyncio import AsyncSession

from dataraum_context.core.models.base import Result
from dataraum_context.entropy.models import ColumnEntropyProfile, CompoundRisk

if TYPE_CHECKING:
    from dataraum_context.llm.cache import LLMCache
    from dataraum_context.llm.config import LLMConfig
    from dataraum_context.llm.prompts import PromptRenderer
    from dataraum_context.llm.providers.base import LLMProvider


@dataclass
class Assumption:
    """An assumption about uncertain data.

    Generated by LLM based on entropy metrics and column context.
    """

    dimension: str  # e.g., "semantic.units", "value.nulls"
    assumption_text: str  # Human-readable assumption
    confidence: str  # "high", "medium", "low"
    impact: str  # What could go wrong if assumption is incorrect
    basis: str = "inferred"  # "inferred", "default", "user_specified"


@dataclass
class ResolutionAction:
    """A suggested action to reduce entropy.

    Generated by LLM with contextual descriptions.
    """

    action: str  # e.g., "add_unit_declaration", "document_null_meaning"
    description: str  # Human-readable description
    priority: str  # "high", "medium", "low"
    effort: str  # "low", "medium", "high"
    expected_impact: str  # What dimensions this will improve
    parameters: dict[str, Any] = field(default_factory=dict)


@dataclass
class EntropyInterpretation:
    """Complete interpretation of a column's entropy profile.

    Contains LLM-generated assumptions, resolution actions, and explanation.
    """

    column_name: str
    table_name: str

    # LLM-generated content
    assumptions: list[Assumption]
    resolution_actions: list[ResolutionAction]
    explanation: str  # Human-readable summary

    # Original metrics for reference
    composite_score: float
    readiness: str

    # Metadata
    model_used: str | None = None
    from_cache: bool = False


@dataclass
class InterpretationInput:
    """Input data for entropy interpretation.

    Combines entropy profile with raw metrics from detectors.
    """

    table_name: str
    column_name: str
    detected_type: str
    business_description: str | None

    # Entropy profile
    composite_score: float
    readiness: str
    structural_entropy: float
    semantic_entropy: float
    value_entropy: float
    computational_entropy: float

    # Raw metrics from detectors
    raw_metrics: dict[str, Any]

    # High entropy dimensions
    high_entropy_dimensions: list[str]

    # Compound risks
    compound_risks: list[CompoundRisk]

    # Optional query context for query-time refinement
    query_context: str | None = None

    @classmethod
    def from_profile(
        cls,
        profile: ColumnEntropyProfile,
        detected_type: str = "unknown",
        business_description: str | None = None,
        raw_metrics: dict[str, Any] | None = None,
        query_context: str | None = None,
    ) -> InterpretationInput:
        """Create input from a ColumnEntropyProfile.

        Args:
            profile: Column entropy profile
            detected_type: Detected data type
            business_description: Business description if available
            raw_metrics: Raw metrics from detectors
            query_context: Optional query context

        Returns:
            InterpretationInput ready for LLM
        """
        return cls(
            table_name=profile.table_name,
            column_name=profile.column_name,
            detected_type=detected_type,
            business_description=business_description,
            composite_score=profile.composite_score,
            readiness=profile.readiness,
            structural_entropy=profile.structural_entropy,
            semantic_entropy=profile.semantic_entropy,
            value_entropy=profile.value_entropy,
            computational_entropy=profile.computational_entropy,
            raw_metrics=raw_metrics or {},
            high_entropy_dimensions=profile.high_entropy_dimensions,
            compound_risks=profile.compound_risks,
            query_context=query_context,
        )


class EntropyInterpreter:
    """LLM-powered entropy interpreter.

    Analyzes entropy metrics and generates contextual assumptions,
    resolution actions, and explanations.

    Note: This class follows the LLMFeature pattern but doesn't inherit
    from it directly to avoid circular imports with entropy module.
    """

    def __init__(
        self,
        config: LLMConfig,
        provider: LLMProvider,
        prompt_renderer: PromptRenderer,
        cache: LLMCache,
    ) -> None:
        """Initialize entropy interpreter.

        Args:
            config: LLM configuration
            provider: LLM provider instance
            prompt_renderer: Prompt template renderer
            cache: Response cache
        """
        self.config = config
        self.provider = provider
        self.renderer = prompt_renderer
        self.cache = cache

    async def interpret(
        self,
        session: AsyncSession,
        input_data: InterpretationInput,
    ) -> Result[EntropyInterpretation]:
        """Interpret entropy metrics for a column.

        Args:
            session: Database session
            input_data: Interpretation input with entropy metrics

        Returns:
            Result containing EntropyInterpretation
        """
        # Check if feature is enabled
        feature_config = self.config.features.entropy_interpretation
        if not feature_config or not feature_config.enabled:
            return Result.fail("Entropy interpretation is disabled in config")

        # Build context for prompt
        context = self._build_prompt_context(input_data)

        # Render prompt
        try:
            prompt, temperature = self.renderer.render("entropy_interpretation", context)
        except Exception as e:
            return Result.fail(f"Failed to render prompt: {e}")

        # Call LLM with caching
        response_result = await self._call_llm(
            session=session,
            feature_name="entropy_interpretation",
            prompt=prompt,
            temperature=temperature,
            model_tier=feature_config.model_tier,
        )

        if not response_result.success:
            return Result.fail(response_result.error or "LLM call failed")

        response = response_result.unwrap()

        # Parse response
        return self._parse_response(input_data, response.content)

    async def interpret_for_query(
        self,
        session: AsyncSession,
        input_data: InterpretationInput,
        query: str,
        aggregations: list[str] | None = None,
        joins: list[str] | None = None,
    ) -> Result[EntropyInterpretation]:
        """Interpret entropy with query-specific context.

        Refines the interpretation based on how the column will be used
        in a specific query.

        Args:
            session: Database session
            input_data: Interpretation input
            query: The SQL query or natural language query
            aggregations: Aggregation operations being performed
            joins: Join operations being performed

        Returns:
            Result containing query-specific EntropyInterpretation
        """
        # Build query context
        query_context_parts = [f"Query: {query}"]
        if aggregations:
            query_context_parts.append(f"Aggregations: {', '.join(aggregations)}")
        if joins:
            query_context_parts.append(f"Joins: {', '.join(joins)}")

        input_data.query_context = "\n".join(query_context_parts)

        return await self.interpret(session, input_data)

    def _build_prompt_context(self, input_data: InterpretationInput) -> dict[str, str]:
        """Build context dictionary for prompt rendering."""
        # Format compound risks
        compound_risks_text = "None"
        if input_data.compound_risks:
            risk_lines = []
            for risk in input_data.compound_risks:
                risk_lines.append(
                    f"- {risk.risk_level.upper()}: {' + '.join(risk.dimensions)}\n"
                    f"  Impact: {risk.impact}\n"
                    f"  Combined score: {risk.combined_score:.2f}"
                )
            compound_risks_text = "\n".join(risk_lines)

        # Format high entropy dimensions
        high_dims_text = "None"
        if input_data.high_entropy_dimensions:
            high_dims_text = "\n".join(f"- {dim}" for dim in input_data.high_entropy_dimensions)

        return {
            "table_name": input_data.table_name,
            "column_name": input_data.column_name,
            "detected_type": input_data.detected_type,
            "business_description": input_data.business_description or "Not documented",
            "composite_score": f"{input_data.composite_score:.2f}",
            "readiness": input_data.readiness,
            "structural_entropy": f"{input_data.structural_entropy:.2f}",
            "semantic_entropy": f"{input_data.semantic_entropy:.2f}",
            "value_entropy": f"{input_data.value_entropy:.2f}",
            "computational_entropy": f"{input_data.computational_entropy:.2f}",
            "raw_metrics_json": json.dumps(input_data.raw_metrics, indent=2, default=str),
            "high_entropy_dimensions": high_dims_text,
            "compound_risks": compound_risks_text,
            "query_context": input_data.query_context
            or "Analysis-time baseline (no specific query context)",
        }

    async def _call_llm(
        self,
        session: AsyncSession,
        feature_name: str,
        prompt: str,
        temperature: float,
        model_tier: str,
    ) -> Result[Any]:
        """Call LLM with caching.

        Mirrors LLMFeature._call_llm but avoids circular import.
        """
        from dataraum_context.llm.providers.base import LLMRequest

        # Get model for tier
        model = self.provider.get_model_for_tier(model_tier)

        # Check cache first
        cached = await self.cache.get(
            session=session,
            feature=feature_name,
            prompt=prompt,
            model=model,
            table_ids=None,
        )

        if cached:
            return Result.ok(cached)

        # Call provider
        request = LLMRequest(
            prompt=prompt,
            max_tokens=self.config.limits.max_output_tokens_per_request,
            temperature=temperature,
            response_format="json",
        )

        result = await self.provider.complete(request)
        if not result.success or not result.value:
            return result

        # Store in cache
        await self.cache.put(
            session=session,
            feature=feature_name,
            prompt=prompt,
            response=result.value,
            source_id=None,
            table_ids=None,
            ontology=None,
            ttl_seconds=self.config.limits.cache_ttl_seconds,
        )

        return result

    def _parse_response(
        self,
        input_data: InterpretationInput,
        response_content: str,
    ) -> Result[EntropyInterpretation]:
        """Parse LLM response into EntropyInterpretation.

        Args:
            input_data: Original input data
            response_content: Raw LLM response

        Returns:
            Result containing parsed EntropyInterpretation
        """
        try:
            # Extract JSON from response
            json_match = re.search(r"\{[\s\S]*\}", response_content)
            if not json_match:
                return Result.fail("No JSON found in response")

            parsed = json.loads(json_match.group())

            # Parse assumptions
            assumptions = []
            for assumption in parsed.get("assumptions", []):
                assumptions.append(
                    Assumption(
                        dimension=assumption.get("dimension", "unknown"),
                        assumption_text=assumption.get("assumption_text", ""),
                        confidence=assumption.get("confidence", "medium"),
                        impact=assumption.get("impact", ""),
                        basis=assumption.get("basis", "inferred"),
                    )
                )

            # Parse resolution actions
            resolution_actions = []
            for action in parsed.get("resolution_actions", []):
                resolution_actions.append(
                    ResolutionAction(
                        action=action.get("action", ""),
                        description=action.get("description", ""),
                        priority=action.get("priority", "medium"),
                        effort=action.get("effort", "medium"),
                        expected_impact=action.get("expected_impact", ""),
                        parameters=action.get("parameters", {}),
                    )
                )

            interpretation = EntropyInterpretation(
                column_name=input_data.column_name,
                table_name=input_data.table_name,
                assumptions=assumptions,
                resolution_actions=resolution_actions,
                explanation=parsed.get("explanation", "No explanation provided"),
                composite_score=input_data.composite_score,
                readiness=input_data.readiness,
            )

            return Result.ok(interpretation)

        except json.JSONDecodeError as e:
            return Result.fail(f"Failed to parse JSON: {e}")
        except Exception as e:
            return Result.fail(f"Failed to parse response: {e}")


def create_fallback_interpretation(
    input_data: InterpretationInput,
) -> EntropyInterpretation:
    """Create a fallback interpretation when LLM is unavailable.

    Generates basic interpretation from entropy profile without LLM.

    Args:
        input_data: Interpretation input

    Returns:
        Basic EntropyInterpretation without LLM-generated content
    """
    # Generate basic assumptions from high entropy dimensions
    assumptions = []
    for dim in input_data.high_entropy_dimensions:
        assumptions.append(
            Assumption(
                dimension=dim,
                assumption_text=f"Uncertainty in {dim} may affect query results",
                confidence="low",
                impact="Results may be unreliable for this dimension",
                basis="default",
            )
        )

    # Generate basic resolution actions
    resolution_actions = []
    if input_data.semantic_entropy > 0.5:
        resolution_actions.append(
            ResolutionAction(
                action="add_documentation",
                description="Add business description and semantic annotations",
                priority="high",
                effort="low",
                expected_impact="Reduces semantic entropy",
            )
        )

    if input_data.value_entropy > 0.5:
        resolution_actions.append(
            ResolutionAction(
                action="investigate_data_quality",
                description="Review null values and outliers",
                priority="medium",
                effort="medium",
                expected_impact="Reduces value entropy",
            )
        )

    # Generate basic explanation
    explanation = (
        f"Column {input_data.column_name} has composite entropy {input_data.composite_score:.2f} "
        f"(readiness: {input_data.readiness}). "
    )
    if input_data.high_entropy_dimensions:
        explanation += f"High uncertainty in: {', '.join(input_data.high_entropy_dimensions)}. "
    if input_data.compound_risks:
        explanation += f"Found {len(input_data.compound_risks)} compound risk(s). "

    return EntropyInterpretation(
        column_name=input_data.column_name,
        table_name=input_data.table_name,
        assumptions=assumptions,
        resolution_actions=resolution_actions,
        explanation=explanation,
        composite_score=input_data.composite_score,
        readiness=input_data.readiness,
    )


__all__ = [
    "Assumption",
    "ResolutionAction",
    "EntropyInterpretation",
    "InterpretationInput",
    "EntropyInterpreter",
    "create_fallback_interpretation",
]

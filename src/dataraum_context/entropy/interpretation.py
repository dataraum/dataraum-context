"""Entropy Interpretation Agent - LLM-powered entropy analysis.

This agent interprets entropy metrics to generate contextual assumptions,
resolution actions, and explanations. It replaces hardcoded heuristics
with LLM-powered semantic interpretation.

See BACKLOG.md Step 2.5.3 for context.
"""

from __future__ import annotations

import json
import re
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any

from sqlalchemy.ext.asyncio import AsyncSession

from dataraum_context.core.models.base import Result
from dataraum_context.entropy.models import ColumnEntropyProfile, CompoundRisk

if TYPE_CHECKING:
    from dataraum_context.llm.cache import LLMCache
    from dataraum_context.llm.config import LLMConfig
    from dataraum_context.llm.prompts import PromptRenderer
    from dataraum_context.llm.providers.base import LLMProvider


@dataclass
class Assumption:
    """An assumption about uncertain data.

    Generated by LLM based on entropy metrics and column context.
    """

    dimension: str  # e.g., "semantic.units", "value.nulls"
    assumption_text: str  # Human-readable assumption
    confidence: str  # "high", "medium", "low"
    impact: str  # What could go wrong if assumption is incorrect
    basis: str = "inferred"  # "inferred", "default", "user_specified"


@dataclass
class ResolutionAction:
    """A suggested action to reduce entropy.

    Generated by LLM with contextual descriptions.
    """

    action: str  # e.g., "add_unit_declaration", "document_null_meaning"
    description: str  # Human-readable description
    priority: str  # "high", "medium", "low"
    effort: str  # "low", "medium", "high"
    expected_impact: str  # What dimensions this will improve
    parameters: dict[str, Any] = field(default_factory=dict)


@dataclass
class EntropyInterpretation:
    """Complete interpretation of a column's entropy profile.

    Contains LLM-generated assumptions, resolution actions, and explanation.
    """

    column_name: str
    table_name: str

    # LLM-generated content
    assumptions: list[Assumption]
    resolution_actions: list[ResolutionAction]
    explanation: str  # Human-readable summary

    # Original metrics for reference
    composite_score: float
    readiness: str

    # Metadata
    model_used: str | None = None
    from_cache: bool = False


@dataclass
class InterpretationInput:
    """Input data for entropy interpretation.

    Combines entropy profile with raw metrics from detectors.
    """

    table_name: str
    column_name: str
    detected_type: str
    business_description: str | None

    # Entropy profile
    composite_score: float
    readiness: str
    structural_entropy: float
    semantic_entropy: float
    value_entropy: float
    computational_entropy: float

    # Raw metrics from detectors
    raw_metrics: dict[str, Any]

    # High entropy dimensions
    high_entropy_dimensions: list[str]

    # Compound risks
    compound_risks: list[CompoundRisk]

    @classmethod
    def from_profile(
        cls,
        profile: ColumnEntropyProfile,
        detected_type: str = "unknown",
        business_description: str | None = None,
        raw_metrics: dict[str, Any] | None = None,
    ) -> InterpretationInput:
        """Create input from a ColumnEntropyProfile.

        Args:
            profile: Column entropy profile
            detected_type: Detected data type
            business_description: Business description if available
            raw_metrics: Raw metrics from detectors

        Returns:
            InterpretationInput ready for LLM
        """
        return cls(
            table_name=profile.table_name,
            column_name=profile.column_name,
            detected_type=detected_type,
            business_description=business_description,
            composite_score=profile.composite_score,
            readiness=profile.readiness,
            structural_entropy=profile.structural_entropy,
            semantic_entropy=profile.semantic_entropy,
            value_entropy=profile.value_entropy,
            computational_entropy=profile.computational_entropy,
            raw_metrics=raw_metrics or {},
            high_entropy_dimensions=profile.high_entropy_dimensions,
            compound_risks=profile.compound_risks,
        )


class EntropyInterpreter:
    """LLM-powered entropy interpreter.

    Analyzes entropy metrics and generates contextual assumptions,
    resolution actions, and explanations.

    Note: This class follows the LLMFeature pattern but doesn't inherit
    from it directly to avoid circular imports with entropy module.
    """

    def __init__(
        self,
        config: LLMConfig,
        provider: LLMProvider,
        prompt_renderer: PromptRenderer,
        cache: LLMCache,
    ) -> None:
        """Initialize entropy interpreter.

        Args:
            config: LLM configuration
            provider: LLM provider instance
            prompt_renderer: Prompt template renderer
            cache: Response cache
        """
        self.config = config
        self.provider = provider
        self.renderer = prompt_renderer
        self.cache = cache

    async def interpret_batch(
        self,
        session: AsyncSession,
        inputs: list[InterpretationInput],
        query: str | None = None,
    ) -> Result[dict[str, EntropyInterpretation]]:
        """Interpret entropy for multiple columns in a single LLM call.

        Makes a single batch LLM call for all columns. If a query is provided,
        the model considers how columns interact in that query context.

        Args:
            session: Database session
            inputs: List of InterpretationInput for each column
            query: Optional SQL or natural language query for context

        Returns:
            Result containing dict mapping column keys to interpretations
        """
        if not inputs:
            return Result.ok({})

        # Choose feature and prompt based on whether query is provided
        if query is not None:
            feature_config = self.config.features.entropy_query_interpretation
            feature_name = "entropy_query_interpretation"
            prompt_name = "entropy_query_interpretation"
        else:
            feature_config = self.config.features.entropy_interpretation
            feature_name = "entropy_interpretation"
            prompt_name = "entropy_interpretation"

        if not feature_config or not feature_config.enabled:
            return Result.fail(f"{feature_name} is disabled in config")

        # Build columns JSON for prompt
        columns_data = []
        for inp in inputs:
            columns_data.append(
                {
                    "key": f"{inp.table_name}.{inp.column_name}",
                    "table_name": inp.table_name,
                    "column_name": inp.column_name,
                    "detected_type": inp.detected_type,
                    "business_description": inp.business_description or "Not documented",
                    "composite_score": inp.composite_score,
                    "readiness": inp.readiness,
                    "structural_entropy": inp.structural_entropy,
                    "semantic_entropy": inp.semantic_entropy,
                    "value_entropy": inp.value_entropy,
                    "computational_entropy": inp.computational_entropy,
                    "high_entropy_dimensions": inp.high_entropy_dimensions,
                    "raw_metrics": inp.raw_metrics,
                    "compound_risks": [
                        {
                            "dimensions": r.dimensions,
                            "risk_level": r.risk_level,
                            "impact": r.impact,
                        }
                        for r in inp.compound_risks
                    ],
                }
            )

        context: dict[str, str] = {
            "columns_json": json.dumps(columns_data, indent=2),
        }
        if query is not None:
            context["query"] = query

        # Render prompt
        try:
            prompt, temperature = self.renderer.render(prompt_name, context)
        except Exception as e:
            return Result.fail(f"Failed to render prompt: {e}")

        # Call LLM
        response_result = await self._call_llm(
            session=session,
            feature_name=feature_name,
            prompt=prompt,
            temperature=temperature,
            model_tier=feature_config.model_tier,
        )

        if not response_result.success:
            return Result.fail(response_result.error or "LLM call failed")

        response = response_result.unwrap()

        # Parse batch response
        return self._parse_batch_response(inputs, response.content)

    def _parse_batch_response(
        self,
        inputs: list[InterpretationInput],
        response_content: str,
    ) -> Result[dict[str, EntropyInterpretation]]:
        """Parse batch LLM response into multiple interpretations.

        Args:
            inputs: Original input data for each column
            response_content: Raw LLM response

        Returns:
            Result containing dict mapping column keys to interpretations
        """
        try:
            # Extract JSON from response
            json_match = re.search(r"\{[\s\S]*\}", response_content)
            if not json_match:
                return Result.fail("No JSON found in response")

            parsed = json.loads(json_match.group())
            columns_data = parsed.get("columns", {})

            # Build lookup for inputs
            inputs_by_key = {f"{inp.table_name}.{inp.column_name}": inp for inp in inputs}

            interpretations: dict[str, EntropyInterpretation] = {}

            for key, col_data in columns_data.items():
                inp = inputs_by_key.get(key)
                if not inp:
                    continue

                # Parse assumptions
                assumptions = []
                for assumption in col_data.get("assumptions", []):
                    assumptions.append(
                        Assumption(
                            dimension=assumption.get("dimension", "unknown"),
                            assumption_text=assumption.get("assumption_text", ""),
                            confidence=assumption.get("confidence", "medium"),
                            impact=assumption.get("impact", ""),
                            basis="inferred",
                        )
                    )

                # Parse resolution actions
                resolution_actions = []
                for action in col_data.get("resolution_actions", []):
                    resolution_actions.append(
                        ResolutionAction(
                            action=action.get("action", ""),
                            description=action.get("description", ""),
                            priority=action.get("priority", "medium"),
                            effort=action.get("effort", "medium"),
                            expected_impact=action.get("expected_impact", ""),
                        )
                    )

                interpretations[key] = EntropyInterpretation(
                    column_name=inp.column_name,
                    table_name=inp.table_name,
                    assumptions=assumptions,
                    resolution_actions=resolution_actions,
                    explanation=col_data.get("explanation", "No explanation provided"),
                    composite_score=inp.composite_score,
                    readiness=inp.readiness,
                )

            return Result.ok(interpretations)

        except json.JSONDecodeError as e:
            return Result.fail(f"Failed to parse JSON: {e}")
        except Exception as e:
            return Result.fail(f"Failed to parse batch response: {e}")

    async def _call_llm(
        self,
        session: AsyncSession,
        feature_name: str,
        prompt: str,
        temperature: float,
        model_tier: str,
    ) -> Result[Any]:
        """Call LLM with caching.

        Mirrors LLMFeature._call_llm but avoids circular import.
        """
        from dataraum_context.llm.providers.base import LLMRequest

        # Get model for tier
        model = self.provider.get_model_for_tier(model_tier)

        # Check cache first
        cached = await self.cache.get(
            session=session,
            feature=feature_name,
            prompt=prompt,
            model=model,
            table_ids=None,
        )

        if cached:
            return Result.ok(cached)

        # Call provider
        request = LLMRequest(
            prompt=prompt,
            max_tokens=self.config.limits.max_output_tokens_per_request,
            temperature=temperature,
            response_format="json",
        )

        result = await self.provider.complete(request)
        if not result.success or not result.value:
            return result

        # Store in cache
        await self.cache.put(
            session=session,
            feature=feature_name,
            prompt=prompt,
            response=result.value,
            source_id=None,
            table_ids=None,
            ontology=None,
            ttl_seconds=self.config.limits.cache_ttl_seconds,
        )

        return result


def create_fallback_interpretation(
    input_data: InterpretationInput,
) -> EntropyInterpretation:
    """Create a fallback interpretation when LLM is unavailable.

    Generates basic interpretation from entropy profile without LLM.

    Args:
        input_data: Interpretation input

    Returns:
        Basic EntropyInterpretation without LLM-generated content
    """
    # Generate basic assumptions from high entropy dimensions
    assumptions = []
    for dim in input_data.high_entropy_dimensions:
        assumptions.append(
            Assumption(
                dimension=dim,
                assumption_text=f"Uncertainty in {dim} may affect query results",
                confidence="low",
                impact="Results may be unreliable for this dimension",
                basis="default",
            )
        )

    # Generate basic resolution actions
    resolution_actions = []
    if input_data.semantic_entropy > 0.5:
        resolution_actions.append(
            ResolutionAction(
                action="add_documentation",
                description="Add business description and semantic annotations",
                priority="high",
                effort="low",
                expected_impact="Reduces semantic entropy",
            )
        )

    if input_data.value_entropy > 0.5:
        resolution_actions.append(
            ResolutionAction(
                action="investigate_data_quality",
                description="Review null values and outliers",
                priority="medium",
                effort="medium",
                expected_impact="Reduces value entropy",
            )
        )

    # Generate basic explanation
    explanation = (
        f"Column {input_data.column_name} has composite entropy {input_data.composite_score:.2f} "
        f"(readiness: {input_data.readiness}). "
    )
    if input_data.high_entropy_dimensions:
        explanation += f"High uncertainty in: {', '.join(input_data.high_entropy_dimensions)}. "
    if input_data.compound_risks:
        explanation += f"Found {len(input_data.compound_risks)} compound risk(s). "

    return EntropyInterpretation(
        column_name=input_data.column_name,
        table_name=input_data.table_name,
        assumptions=assumptions,
        resolution_actions=resolution_actions,
        explanation=explanation,
        composite_score=input_data.composite_score,
        readiness=input_data.readiness,
    )


__all__ = [
    "Assumption",
    "ResolutionAction",
    "EntropyInterpretation",
    "InterpretationInput",
    "EntropyInterpreter",
    "create_fallback_interpretation",
]

name: "graph_sql_generation"
version: "2.0"
description: "Generate executable SQL from a graph specification and data schema"
temperature: 0.0

# System prompt - defines role and behavior
system_prompt: |
  <role>
  You are a financial data analyst specializing in SQL generation from transformation graphs.
  Your task is to generate executable DuckDB SQL that implements graph specifications.
  </role>

  <capabilities>
  - Map abstract fields in graphs to concrete database columns
  - INFER derived fields when direct mapping unavailable (e.g., revenue from transaction amounts)
  - Generate standalone SQL for each step (executed as temp views)
  - Handle German accounting column names (Konto, Betrag, etc.)
  - Use account codes (SKR03, SKR04) in WHERE clauses
  </capabilities>

  <graph_types>
  - filter: Generate SQL that classifies rows (clean/quarantine/exclude)
  - metric: Generate SQL that calculates scalar or aggregated values
  </graph_types>

  <field_resolution_strategy>
  When a graph references a standard_field (e.g., "revenue", "accounts_receivable"):
  1. First check field_mappings for direct business_concept â†’ column mapping
  2. If not directly mapped, analyze the dataset_context to infer the calculation:
     - Look for columns with related semantic_role (e.g., "measure", "amount")
     - Check business_concept annotations for clues
     - Use transaction_type or similar columns to filter relevant rows
     - Example: revenue = SUM(amount) WHERE transaction_type IN ('Invoice', 'Sale')
  3. Document your inference in the column_mappings with explanation
  </field_resolution_strategy>

  <guidelines>
  - Use the accounting notes and mappings in the graph to understand field meanings
  - Generate only valid DuckDB SQL
  - Map abstract fields to concrete columns based on semantic understanding
  - Implement each graph step as standalone SQL returning a single value
  - Final SQL combines step results (can reference step views directly)
  - Quote column names with spaces or special characters using double quotes
  </guidelines>

  <reserved_words>
  NEVER use SQL reserved words as column aliases:
  - CURRENT_DATE, CURRENT_TIME, CURRENT_TIMESTAMP, DATE, TIME, YEAR, MONTH, DAY
  - Use descriptive alternatives: current_date_value, calculation_date, period_days
  </reserved_words>

  <step_execution_model>
  CRITICAL - How steps are executed:
  1. Each step's SQL becomes a temp view: CREATE TEMP VIEW <step_id> AS <sql>
  2. Steps execute sequentially, each creating a named view
  3. Final SQL can reference these views by step_id

  Therefore:
  - Step SQL must be STANDALONE (executable independently)
  - Step SQL must NOT contain WITH clauses or reference other steps
  - Step SQL should return a single value or simple result
  - Final SQL combines step results, not redefine them
  </step_execution_model>

  Use the generate_sql tool to provide your structured response.

# User prompt - provides graph and schema context
user_prompt: |
  <task>
  Generate executable SQL for the following graph specification.
  </task>

  <graph_specification type="{graph_type}">
  {graph_yaml}
  </graph_specification>

  <data_schema>
  {table_schema}
  </data_schema>

  <parameters>
  {parameters}
  </parameters>

  <dataset_context>
  {rich_context}
  </dataset_context>

  <field_mappings>
  {field_mappings}
  </field_mappings>

  <data_quality>
  {entropy_warnings}
  </data_quality>

  <instructions>
  Generate SQL that:
  1. Maps abstract fields (standard_field references) to concrete columns using:
     - Direct mappings from field_mappings above
     - Inferred calculations when direct mapping unavailable (e.g., revenue = SUM(amount) WHERE transaction_type = 'Sale')
  2. Use semantic annotations (business_concept, semantic_role) to understand column purposes
  3. Implements each step as standalone SQL (no CTEs, no step references)
  4. Returns the final result with proper column names

  IMPORTANT: If a standard_field like 'revenue' is not directly mapped, analyze the available columns
  and their business_concepts to construct an appropriate aggregation query.

  STEP FORMAT REQUIREMENTS:
  - Each step SQL must be standalone (no WITH clauses, no step references)
  - Each step returns a scalar value accessible via: SELECT * FROM <step_id>
  - Step IDs must be valid SQL identifiers (no spaces, no reserved words)

  FINAL SQL REQUIREMENTS:
  - Reference step results directly: SELECT step_1.value / step_2.value
  - OR use subqueries: SELECT (SELECT value FROM step_1) / (SELECT value FROM step_2)
  - Do NOT redefine step logic with CTEs

  EXAMPLE:
  steps:
    - step_id: "total_revenue"
      sql: "SELECT SUM(\"Amount\") as value FROM typed_table WHERE \"Type\" = 'Sale'"
    - step_id: "total_ar"
      sql: "SELECT SUM(\"Balance\") as value FROM typed_table WHERE \"Account\" LIKE '12%'"
    - step_id: "period_days"
      sql: "SELECT 30 as value"

  final_sql: "SELECT (SELECT value FROM total_ar) / (SELECT value FROM total_revenue) * (SELECT value FROM period_days) AS dso"
  </instructions>

  Use the generate_sql tool to provide the SQL steps, final SQL, and column mappings.

# Input variable definitions
inputs:
  graph_yaml:
    type: "string"
    required: true
    description: "Full YAML graph specification with accounting context"

  graph_type:
    type: "string"
    required: true
    description: "Type of graph: 'filter' or 'metric'"

  table_schema:
    type: "string"
    required: true
    description: "JSON describing actual table columns, types, and sample values"

  parameters:
    type: "string"
    required: false
    default: "{}"
    description: "JSON of parameter values to use"

  rich_context:
    type: "string"
    required: false
    default: ""
    description: "Dataset context with tables, columns, semantic annotations, relationships"

  field_mappings:
    type: "string"
    required: false
    default: ""
    description: "Business concept to column mappings from semantic analysis"

  entropy_warnings:
    type: "string"
    required: false
    default: "Data quality assessment not available - proceed with caution."
    description: "Data quality warnings and entropy scores"

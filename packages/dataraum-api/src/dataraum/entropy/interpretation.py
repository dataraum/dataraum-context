"""Entropy Interpretation Agent - LLM-powered entropy analysis.

This agent interprets entropy metrics to generate contextual assumptions,
resolution actions, and explanations. It replaces hardcoded heuristics
with LLM-powered semantic interpretation.

See BACKLOG.md Step 2.5.3 for context.
"""

from __future__ import annotations

import json
from dataclasses import dataclass, field
from typing import TYPE_CHECKING, Any, Literal

from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from dataraum.core.models.base import Result
from dataraum.entropy.analysis.aggregator import ColumnSummary
from dataraum.entropy.models import CompoundRisk

if TYPE_CHECKING:
    from dataraum.llm.cache import LLMCache
    from dataraum.llm.config import LLMConfig
    from dataraum.llm.prompts import PromptRenderer
    from dataraum.llm.providers.base import LLMProvider


@dataclass
class Assumption:
    """An assumption about uncertain data.

    Generated by LLM based on entropy metrics and column context.
    """

    dimension: str  # e.g., "semantic.units", "value.nulls"
    assumption_text: str  # Human-readable assumption
    confidence: str  # "high", "medium", "low"
    impact: str  # What could go wrong if assumption is incorrect
    basis: str = "inferred"  # "inferred", "default", "user_specified"


@dataclass
class ResolutionAction:
    """A suggested action to reduce entropy.

    Generated by LLM with contextual descriptions.
    """

    action: str  # e.g., "add_unit_declaration", "document_null_meaning"
    description: str  # Human-readable description
    priority: str  # "high", "medium", "low"
    effort: str  # "low", "medium", "high"
    expected_impact: str  # What dimensions this will improve
    parameters: dict[str, Any] = field(default_factory=dict)


@dataclass
class EntropyInterpretation:
    """Complete interpretation of a column's entropy profile.

    Contains LLM-generated assumptions, resolution actions, and explanation.
    """

    column_name: str
    table_name: str

    # LLM-generated content
    assumptions: list[Assumption]
    resolution_actions: list[ResolutionAction]
    explanation: str  # Human-readable summary

    # Original metrics for reference
    composite_score: float
    readiness: str

    # Metadata
    model_used: str | None = None
    from_cache: bool = False

    def to_dashboard_dict(self) -> dict[str, Any]:
        """Convert interpretation to dashboard-friendly dictionary.

        Returns a clean structure suitable for JSON serialization and UI display.
        """
        return {
            "column_key": f"{self.table_name}.{self.column_name}",
            "table_name": self.table_name,
            "column_name": self.column_name,
            "explanation": self.explanation,
            "composite_score": round(self.composite_score, 2),
            "readiness": self.readiness,
            "assumptions": [
                {
                    "dimension": a.dimension,
                    "text": a.assumption_text,
                    "confidence": a.confidence,
                    "impact": a.impact,
                }
                for a in self.assumptions
            ],
            "resolution_actions": [
                {
                    "action": r.action,
                    "description": r.description,
                    "priority": r.priority,
                    "effort": r.effort,
                }
                for r in self.resolution_actions
            ],
            "metadata": {
                "model_used": self.model_used,
                "from_cache": self.from_cache,
            },
        }


@dataclass
class InterpretationInput:
    """Input data for entropy interpretation.

    Combines entropy profile with raw metrics from detectors.
    """

    table_name: str
    column_name: str
    detected_type: str
    business_description: str | None

    # Entropy profile
    composite_score: float
    readiness: str
    structural_entropy: float
    semantic_entropy: float
    value_entropy: float
    computational_entropy: float

    # Raw metrics from detectors
    raw_metrics: dict[str, Any]

    # High entropy dimensions
    high_entropy_dimensions: list[str]

    # Compound risks
    compound_risks: list[CompoundRisk]

    @classmethod
    def from_summary(
        cls,
        summary: ColumnSummary,
        detected_type: str = "unknown",
        business_description: str | None = None,
        raw_metrics: dict[str, Any] | None = None,
    ) -> InterpretationInput:
        """Create input from a ColumnSummary.

        Args:
            summary: Column entropy summary
            detected_type: Detected data type
            business_description: Business description if available
            raw_metrics: Raw metrics from detectors

        Returns:
            InterpretationInput ready for LLM
        """
        return cls(
            table_name=summary.table_name,
            column_name=summary.column_name,
            detected_type=detected_type,
            business_description=business_description,
            composite_score=summary.composite_score,
            readiness=summary.readiness,
            structural_entropy=summary.layer_scores.get("structural", 0.0),
            semantic_entropy=summary.layer_scores.get("semantic", 0.0),
            value_entropy=summary.layer_scores.get("value", 0.0),
            computational_entropy=summary.layer_scores.get("computational", 0.0),
            raw_metrics=raw_metrics or {},
            high_entropy_dimensions=summary.high_entropy_dimensions,
            compound_risks=summary.compound_risks,
        )


# =============================================================================
# Pydantic models for LLM tool output
# =============================================================================


class AssumptionOutput(BaseModel):
    """Pydantic model for assumption in LLM tool output."""

    dimension: str = Field(
        description="Entropy dimension this assumption addresses, e.g., 'semantic.units', 'value.nulls'"
    )
    assumption_text: str = Field(description="Human-readable assumption statement")
    confidence: Literal["high", "medium", "low"] = Field(
        description="Confidence level in this assumption"
    )
    impact: str = Field(description="What could go wrong if this assumption is incorrect")


class ResolutionActionOutput(BaseModel):
    """Pydantic model for resolution action in LLM tool output."""

    action: str = Field(
        description="Action identifier, e.g., 'add_unit_declaration', 'document_null_meaning'"
    )
    description: str = Field(description="Human-readable description of the action")
    priority: Literal["high", "medium", "low"] = Field(description="Priority of this action")
    effort: Literal["low", "medium", "high"] = Field(description="Effort required to implement")
    expected_impact: str = Field(description="What entropy dimensions this will improve")


class ColumnInterpretationOutput(BaseModel):
    """Pydantic model for a single column interpretation in LLM tool output."""

    assumptions: list[AssumptionOutput] = Field(
        default_factory=list, description="List of assumptions about this column's data"
    )
    resolution_actions: list[ResolutionActionOutput] = Field(
        default_factory=list, description="List of suggested actions to reduce uncertainty"
    )
    explanation: str = Field(
        description="Brief human-readable explanation of the data quality situation"
    )


class EntropyInterpretationOutput(BaseModel):
    """Pydantic model for LLM tool output - batch entropy interpretation.

    Used as a tool definition for structured LLM output via tool use API.
    """

    columns: dict[str, ColumnInterpretationOutput] = Field(
        description="Interpretations keyed by 'table_name.column_name'"
    )


class EntropyInterpreter:
    """LLM-powered entropy interpreter.

    Analyzes entropy metrics and generates contextual assumptions,
    resolution actions, and explanations using structured tool output.

    Note: This class follows the LLMFeature pattern but doesn't inherit
    from it directly to avoid circular imports with entropy module.
    """

    def __init__(
        self,
        config: LLMConfig,
        provider: LLMProvider,
        prompt_renderer: PromptRenderer,
        cache: LLMCache,
    ) -> None:
        """Initialize entropy interpreter.

        Args:
            config: LLM configuration
            provider: LLM provider instance
            prompt_renderer: Prompt template renderer
            cache: Response cache
        """
        self.config = config
        self.provider = provider
        self.renderer = prompt_renderer
        self.cache = cache

    def interpret_batch(
        self,
        session: Session,
        inputs: list[InterpretationInput],
        query: str | None = None,
    ) -> Result[dict[str, EntropyInterpretation]]:
        """Interpret entropy for multiple columns in a single LLM call.

        Makes a single batch LLM call for all columns using tool-based output.
        If a query is provided, the model considers how columns interact in that context.

        Args:
            session: Database session
            inputs: List of InterpretationInput for each column
            query: Optional SQL or natural language query for context

        Returns:
            Result containing dict mapping column keys to interpretations
        """
        from dataraum.llm.providers.base import (
            ConversationRequest,
            Message,
            ToolDefinition,
        )

        if not inputs:
            return Result.fail("No inputs provided for interpretation")

        # Choose feature and prompt based on whether query is provided
        if query is not None:
            feature_config = self.config.features.entropy_query_interpretation
            feature_name = "entropy_query_interpretation"
            prompt_name = "entropy_query_interpretation"
        else:
            feature_config = self.config.features.entropy_interpretation
            feature_name = "entropy_interpretation"
            prompt_name = "entropy_interpretation"

        if not feature_config or not feature_config.enabled:
            return Result.fail(f"{feature_name} is disabled in config")

        # Build columns JSON for prompt
        columns_data = []
        for inp in inputs:
            columns_data.append(
                {
                    "key": f"{inp.table_name}.{inp.column_name}",
                    "table_name": inp.table_name,
                    "column_name": inp.column_name,
                    "detected_type": inp.detected_type,
                    "business_description": inp.business_description or "Not documented",
                    "composite_score": round(inp.composite_score, 3),
                    "readiness": inp.readiness,
                    "structural_entropy": round(inp.structural_entropy, 3),
                    "semantic_entropy": round(inp.semantic_entropy, 3),
                    "value_entropy": round(inp.value_entropy, 3),
                    "computational_entropy": round(inp.computational_entropy, 3),
                    "high_entropy_dimensions": inp.high_entropy_dimensions,
                    "raw_metrics": inp.raw_metrics,
                    "compound_risks": [
                        {
                            "dimensions": r.dimensions,
                            "risk_level": r.risk_level,
                            "impact": r.impact,
                        }
                        for r in inp.compound_risks
                    ],
                }
            )

        context: dict[str, str] = {
            "columns_json": json.dumps(columns_data, indent=2),
        }
        if query is not None:
            context["query"] = query

        # Render prompt with system/user split
        try:
            system_prompt, user_prompt, temperature = self.renderer.render_split(
                prompt_name, context
            )
        except Exception as e:
            return Result.fail(f"Failed to render prompt: {e}")

        # Define tool for structured output
        tool = ToolDefinition(
            name="interpret_entropy",
            description="Provide structured interpretation of entropy metrics for the columns",
            input_schema=EntropyInterpretationOutput.model_json_schema(),
        )

        model = self.provider.get_model_for_tier(feature_config.model_tier)

        # Call LLM with tool use
        request = ConversationRequest(
            messages=[Message(role="user", content=user_prompt)],
            system=system_prompt,
            tools=[tool],
            max_tokens=self.config.limits.max_output_tokens_per_request,
            temperature=temperature,
        )

        # Retry logic for transient failures
        max_retries = 2
        last_error = None

        for _attempt in range(max_retries + 1):
            result = self.provider.converse(request)
            if not result.success or not result.value:
                last_error = result.error or "LLM call failed"
                continue

            response = result.value

            # Extract tool call result
            if not response.tool_calls:
                # LLM didn't use the tool - try to parse text as fallback
                if response.content:
                    try:
                        response_data = json.loads(response.content)
                        output = EntropyInterpretationOutput.model_validate(response_data)
                        return self._convert_output_to_interpretations(inputs, output, model)
                    except Exception:
                        last_error = f"LLM did not use tool. Response: {response.content[:200]}"
                        continue
                else:
                    last_error = "LLM did not use the interpret_entropy tool"
                    continue

            # Parse tool response using Pydantic model
            tool_call = response.tool_calls[0]
            if tool_call.name != "interpret_entropy":
                last_error = f"Unexpected tool call: {tool_call.name}"
                continue

            try:
                output = EntropyInterpretationOutput.model_validate(tool_call.input)
                # Success - return the result
                return self._convert_output_to_interpretations(inputs, output, model)
            except Exception as e:
                last_error = f"Failed to validate tool response: {e}"
                continue

        # All retries failed
        return Result.fail(last_error or "All retry attempts failed")

    def _convert_output_to_interpretations(
        self,
        inputs: list[InterpretationInput],
        output: EntropyInterpretationOutput,
        model: str,
    ) -> Result[dict[str, EntropyInterpretation]]:
        """Convert Pydantic tool output to EntropyInterpretation dataclasses.

        Args:
            inputs: Original input data for each column
            output: Validated Pydantic output from LLM
            model: Model name used for generation

        Returns:
            Result containing dict mapping column keys to interpretations
        """
        # Build lookup for inputs
        inputs_by_key = {f"{inp.table_name}.{inp.column_name}": inp for inp in inputs}

        interpretations: dict[str, EntropyInterpretation] = {}

        for key, col_output in output.columns.items():
            inp = inputs_by_key.get(key)
            if not inp:
                continue

            # Convert assumptions
            assumptions = [
                Assumption(
                    dimension=a.dimension,
                    assumption_text=a.assumption_text,
                    confidence=a.confidence,
                    impact=a.impact,
                    basis="inferred",
                )
                for a in col_output.assumptions
            ]

            # Convert resolution actions
            resolution_actions = [
                ResolutionAction(
                    action=r.action,
                    description=r.description,
                    priority=r.priority,
                    effort=r.effort,
                    expected_impact=r.expected_impact,
                )
                for r in col_output.resolution_actions
            ]

            interpretations[key] = EntropyInterpretation(
                column_name=inp.column_name,
                table_name=inp.table_name,
                assumptions=assumptions,
                resolution_actions=resolution_actions,
                explanation=col_output.explanation,
                composite_score=inp.composite_score,
                readiness=inp.readiness,
                model_used=model,
                from_cache=False,
            )

        return Result.ok(interpretations)


__all__ = [
    "Assumption",
    "ResolutionAction",
    "EntropyInterpretation",
    "InterpretationInput",
    "EntropyInterpreter",
    # Pydantic output models for tool use
    "AssumptionOutput",
    "ResolutionActionOutput",
    "ColumnInterpretationOutput",
    "EntropyInterpretationOutput",
]

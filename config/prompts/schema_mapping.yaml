# Schema Mapping Prompt Configuration
# ====================================
#
# LLM prompt for mapping concrete columns to abstract calculation fields.
# Used to understand which columns feed into which financial calculations.

prompt_id: schema_mapping
version: "1.0"
model_recommendations:
  preferred: "claude-sonnet-4-5-20250929"
  fallback: "claude-3-5-haiku-20241022"

# System prompt establishes the LLM's role
system_prompt: |
  You are a financial data analyst expert specializing in mapping database schemas
  to standardized financial calculation fields.

  Your task is to analyze column metadata and determine which abstract financial
  fields each column contributes to. You understand:

  - Financial statement structure (balance sheet, income statement, cash flow)
  - Common accounting terminology in multiple languages (English, German, etc.)
  - Data aggregation patterns (SUM for flows, END_OF_PERIOD for stocks)
  - Sign conventions (assets positive, liabilities positive, expenses negative revenue)

  You provide structured mappings with confidence scores and reasoning.

# User prompt template with placeholders
user_prompt_template: |
  ## Task: Map Columns to Abstract Financial Fields

  I need you to analyze the following database columns and determine which
  abstract financial fields they map to.

  ### Available Abstract Fields

  These are the standard fields used in financial calculations:

  {abstract_fields_description}

  ### Columns to Map

  {columns_to_map}

  ### Instructions

  For each column:
  1. Determine which abstract field(s) it maps to (if any)
  2. Identify the appropriate aggregation method:
     - `sum` - For flow metrics (revenue, expenses, cash flows)
     - `end_of_period` - For stock metrics (balances, receivables, payables)
     - `average` - For metrics needing period averages
  3. Note any filter conditions needed (e.g., only include certain transaction types)
  4. Provide confidence (0.0-1.0) and reasoning

  If a column doesn't map to any abstract field, mark it as "unmapped" with reasoning.

  Use the `map_column_to_field` tool for EACH column.

# Tool definition for structured output
tool_definition:
  name: map_column_to_field
  description: Map a database column to an abstract financial field
  input_schema:
    type: object
    required:
      - table_name
      - column_name
      - abstract_field
      - confidence
      - reasoning
    properties:
      table_name:
        type: string
        description: Source table name
      column_name:
        type: string
        description: Source column name
      abstract_field:
        type: string
        description: |
          Target abstract field (e.g., 'revenue', 'accounts_receivable').
          Use 'unmapped' if column doesn't map to any standard field.
      aggregation_method:
        type: string
        enum: ["sum", "end_of_period", "average", "count", "min", "max", "none"]
        description: How to aggregate this column's values
      filter_condition:
        type: string
        description: |
          Optional SQL WHERE condition to filter rows
          (e.g., "transaction_type = 'sale'")
      sign_adjustment:
        type: integer
        enum: [-1, 1]
        default: 1
        description: Multiply by -1 for contra accounts or sign reversal
      confidence:
        type: number
        minimum: 0.0
        maximum: 1.0
        description: Confidence in this mapping (0.0 to 1.0)
      reasoning:
        type: string
        description: Explanation of why this mapping was chosen

# Format helpers for constructing prompts
format_helpers:
  # How to format abstract fields for the prompt
  abstract_field_template: |
    **{field_id}** ({statement})
      Description: {description}
      Aggregation: {aggregation}
      Required: {required}

  # How to format columns for the prompt
  column_template: |
    {index}. **{table}.{column}**
       Type: {data_type}
       Sample values: {samples}
       Null ratio: {null_ratio}
       {semantic_hint}

# Response parsing configuration
response_parsing:
  # Minimum confidence to accept a mapping
  min_confidence: 0.5
  # How to handle unmapped columns
  unmapped_handling: "flag_for_review"
  # Maximum columns per LLM call (to avoid token limits)
  batch_size: 25

# Caching configuration
caching:
  # Cache mappings for reuse
  enabled: true
  # Cache key includes schema hash to invalidate on schema changes
  key_includes:
    - dataset_id
    - schema_hash
    - prompt_version

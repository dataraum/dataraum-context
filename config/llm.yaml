# LLM Configuration
#
# Configure LLM providers for intelligent analysis features.
# When disabled, manual YAML configuration is required.

version: "1.0.0"

# Provider configurations
providers:
  anthropic:
    api_key_env: ANTHROPIC_API_KEY
    default_model: claude-sonnet-4-20250514
    models:
      fast: claude-haiku-4-20250414
      balanced: claude-sonnet-4-20250514

  openai:
    api_key_env: OPENAI_API_KEY
    default_model: gpt-4o
    models:
      fast: gpt-4o-mini
      balanced: gpt-4o

  local:
    # OpenAI-compatible local endpoint (e.g., Ollama, vLLM)
    base_url_env: LOCAL_LLM_BASE_URL
    api_key_env: LOCAL_LLM_API_KEY # Often "not-needed" for local
    default_model: llama3
    models:
      fast: llama3
      balanced: llama3

# Active provider (anthropic, openai, local)
active_provider: anthropic

# Feature toggles
features:
  semantic_analysis:
    enabled: true
    model_tier: balanced
    prompt_file: prompts/semantic_analysis.yaml
    description: >
      Analyze column roles, entity types, business terms,
      and relationships across tables.

  quality_rule_generation:
    enabled: true
    model_tier: balanced
    prompt_file: prompts/quality_rules.yaml
    description: >
      Generate domain-specific quality rules based on
      semantic understanding and ontology.

  suggested_queries:
    enabled: true
    model_tier: fast
    prompt_file: prompts/suggested_queries.yaml
    description: >
      Generate useful starting queries for data exploration.

  context_summary:
    enabled: true
    model_tier: fast
    prompt_file: prompts/context_summary.yaml
    description: >
      Generate natural language overview of the dataset.

  filter_recommendations:
    enabled: true
    model_tier: fast
    prompt_file: prompts/filter_recommendations.yaml
    description: >
      Generate SQL WHERE clauses to filter problematic data
      based on quality issues detected.

# Fallback when LLM disabled or unavailable
fallback:
  semantic_analysis: manual_only # Require config/semantic_overrides.yaml
  quality_rule_generation: default_rules # Use config/rules/default.yaml
  suggested_queries: skip # Don't generate
  context_summary: skip # Don't generate

# Cost and rate controls
limits:
  max_input_tokens_per_request: 8000
  max_output_tokens_per_request: 4000
  max_columns_per_batch: 30
  max_requests_per_minute: 20
  cache_ttl_seconds: 86400 # 24 hours

# Privacy settings for data sent to LLM
privacy:
  # Sample values sent to LLM for analysis
  max_sample_values: 10

  # Use SDV to generate synthetic samples instead of real data
  # Requires: pip install sdv
  use_synthetic_samples: false
  synthetic_sample_count: 20

  # Columns matching these patterns are never sent to external LLM
  # (synthetic data generated instead if use_synthetic_samples=true)
  sensitive_patterns:
    - ".*email.*"
    - ".*phone.*"
    - ".*ssn.*"
    - ".*social_security.*"
    - ".*credit_card.*"
    - ".*password.*"
    - ".*secret.*"
    - ".*token.*"
    - ".*api_key.*"
